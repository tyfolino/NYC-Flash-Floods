{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4692b8",
   "metadata": {},
   "source": [
    "# Notebook for SOM Training\n",
    "\n",
    "By: Ty Janoski\n",
    "\n",
    "Updated 1/11/2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efd764",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb4540",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7986ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmweather  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots  # noqa: F401\n",
    "import xarray as xr\n",
    "from minisom import MiniSom\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "plt.style.use([\"science\", \"nature\", \"grid\"])\n",
    "plt.rcParams[\"text.usetex\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a515db",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b46a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Z500 at flash-flood event times\n",
    "path = \"/mnt/drive2/SOM_intermediate_files/\"\n",
    "\n",
    "# Z500\n",
    "Z500_daily = xr.load_dataarray(f\"{path}era5_Z500_daily.nc\")\n",
    "Z500_norm_daily = xr.load_dataarray(f\"{path}era5_Z500_norm_daily.nc\")\n",
    "Z500_norm_weighted_daily = xr.load_dataarray(f\"{path}era5_Z500_norm_weighted_daily.nc\")\n",
    "\n",
    "# IVT\n",
    "IVT_daily = xr.load_dataset(f\"{path}era5_ivt_daily.nc\")[\"ivt\"]\n",
    "IVT_norm_daily = xr.load_dataset(f\"{path}era5_ivt_norm_daily.nc\")[\"ivt\"]\n",
    "IVT_norm_weighted_daily = xr.load_dataset(f\"{path}era5_ivt_norm_weighted_daily.nc\")[\n",
    "    \"ivt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5206a1",
   "metadata": {},
   "source": [
    "### Reshape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7b2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data for SOM training\n",
    "Z500_flat = Z500_norm_weighted_daily.stack(\n",
    "    features=[\"lat\", \"lon\"]\n",
    ").values  # shape: (time, space)\n",
    "IVT_flat = IVT_norm_weighted_daily.stack(\n",
    "    features=[\"latitude\", \"longitude\"]\n",
    ").values  # shape: (time, space)\n",
    "\n",
    "X = np.concatenate([Z500_flat, IVT_flat], axis=1)  # shape: (time, space*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80c4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read flash flood events and filter to unique episodes\n",
    "df = pd.read_csv(\"data/storm_data_search_results.csv\")\n",
    "df = df[df[\"EVENT_ID\"].astype(str).str.isdigit()].drop_duplicates(\n",
    "    subset=[\"EPISODE_ID\"], keep=\"first\"\n",
    ")\n",
    "\n",
    "# Parse begin datetime: combine date and time, convert to UTC\n",
    "df[\"BEGIN_DATETIME\"] = (\n",
    "    pd.to_datetime(\n",
    "        df[\"BEGIN_DATE\"]\n",
    "        + \" \"\n",
    "        + df[\"BEGIN_TIME\"].fillna(0).astype(int).astype(str).str.zfill(4),\n",
    "        format=\"%m/%d/%Y %H%M\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    .dt.tz_localize(\"US/Eastern\", ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "    .dt.tz_convert(\"UTC\")\n",
    ")\n",
    "\n",
    "# Extract unique event days (timezone-naive for xarray compatibility)\n",
    "event_days = sorted(df[\"BEGIN_DATETIME\"].dt.floor(\"D\").dt.tz_localize(None).unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44096767",
   "metadata": {},
   "source": [
    "## SOM Training\n",
    "\n",
    "We are going to train our SOM with random initialization and online training. We will also use two phases: a \"coarse\" phase with a larger sigma and learning rate, then a \"fine\" phase with a smaller learning rate and sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1c099",
   "metadata": {},
   "source": [
    "### Set SOM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57aa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SOM shape\n",
    "xdim, ydim = 5, 4\n",
    "\n",
    "# Set number of iterations for each phase\n",
    "n1, n2 = 20000, 30000\n",
    "\n",
    "# Set starting sigmas\n",
    "sig1, sig2 = 0.6 * np.sqrt(xdim**2 + ydim**2), 2.0\n",
    "\n",
    "# Set starting learning rates\n",
    "lr1, lr2 = 0.3, 0.1\n",
    "\n",
    "# Random seed for reproducibility\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd0c59",
   "metadata": {},
   "source": [
    "### Train SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fe49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 20000 / 20000 ] 100% - 0:00:00 left \n",
      " quantization error: 115.35613821769022\n",
      "0.0001874062968515742\n",
      " [ 30000 / 30000 ] 100% - 0:00:00 left \n",
      " quantization error: 114.37786864359009\n",
      "0.0020614692653673165\n"
     ]
    }
   ],
   "source": [
    "# Create SOM instance\n",
    "som = MiniSom(\n",
    "    xdim,\n",
    "    ydim,\n",
    "    input_len=X.shape[1],\n",
    "    sigma=sig1,\n",
    "    learning_rate=lr1,\n",
    "    decay_function=\"linear_decay_to_zero\",\n",
    "    sigma_decay_function=\"linear_decay_to_one\",\n",
    "    neighborhood_function=\"gaussian\",\n",
    "    random_seed=random_seed,\n",
    ")\n",
    "\n",
    "# Initialize random weights\n",
    "som.random_weights_init(X)\n",
    "\n",
    "# Random training\n",
    "som.train_random(X, n1, verbose=True)\n",
    "print(som.topographic_error(X))\n",
    "\n",
    "# Phase 2\n",
    "som._sigma = sig2  # type: ignore\n",
    "som._learning_rate = lr2\n",
    "som.train_random(X, n2, verbose=True)\n",
    "print(som.topographic_error(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f85390",
   "metadata": {},
   "source": [
    "### Grab important fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e5a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janoski/miniforge3/envs/soms314/lib/python3.14/site-packages/sklearn/manifold/_mds.py:754: FutureWarning: The default value of `init` will change from 'random' to 'classical_mds' in 1.10. To suppress this warning, provide some value of `init`.\n",
      "  warnings.warn(\n",
      "/home/janoski/miniforge3/envs/soms314/lib/python3.14/site-packages/sklearn/manifold/_mds.py:771: FutureWarning: The `dissimilarity` parameter is deprecated and will be removed in 1.10. Use `metric` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Total node number\n",
    "n_nodes = xdim * ydim\n",
    "\n",
    "# Get flattened weights\n",
    "weights = som.get_weights().reshape(xdim * ydim, -1)\n",
    "\n",
    "# u-matrix\n",
    "u_matrix = som.distance_map().T\n",
    "\n",
    "# bmus & hit_map\n",
    "bmus = np.array([som.winner(x) for x in X])\n",
    "\n",
    "hit_map = np.zeros((xdim, ydim))\n",
    "for i, j in bmus:\n",
    "    hit_map[i, j] += 1\n",
    "hit_map = hit_map.T\n",
    "\n",
    "# Sammon Coordinates\n",
    "D = pairwise_distances(weights)\n",
    "coords = MDS(\n",
    "    n_components=2, dissimilarity=\"precomputed\", random_state=42, n_init=4\n",
    ").fit_transform(D)\n",
    "\n",
    "# Get lats/lons\n",
    "lat = Z500_norm_weighted_daily.lat\n",
    "lon = Z500_norm_weighted_daily.lon\n",
    "\n",
    "# Dimensions of the spatial field\n",
    "n_lat = lat.size\n",
    "n_lon = lon.size\n",
    "n_features = n_lat * n_lon\n",
    "\n",
    "# Split weights into Z500 and IVT components\n",
    "z500_weights = weights[:, :n_features]\n",
    "ivt_weights = weights[:, n_features:]\n",
    "\n",
    "# Reshape weights back to spatial dimensions\n",
    "z500_nodes = z500_weights.reshape(xdim, ydim, n_lat, n_lon)\n",
    "ivt_nodes = ivt_weights.reshape(xdim, ydim, n_lat, n_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666d0b8",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6a70f",
   "metadata": {},
   "source": [
    "### U-matrix and Sammon Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8563653",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, layout=\"constrained\", figsize=(6, 3), dpi=600)\n",
    "\n",
    "# u-matrix\n",
    "im0 = axes[0].imshow(u_matrix, cmap=\"viridis\", origin=\"lower\")\n",
    "axes[0].set_title(\"U-Matrix (Mean Inter-Node Distance)\", fontsize=7)\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04, shrink=0.7)\n",
    "\n",
    "# hit map\n",
    "im1 = axes[1].imshow(hit_map, cmap=\"plasma\", origin=\"lower\")\n",
    "axes[1].set_title(\"Hit Map (Samples per Node)\", fontsize=7)\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04, shrink=0.7)\n",
    "\n",
    "# axis styling\n",
    "for ax in axes:\n",
    "    ax.set_xticks(np.arange(xdim))\n",
    "    ax.set_yticks(np.arange(ydim))\n",
    "    ax.set_xlabel(\"X-index\", fontsize=6)\n",
    "    ax.set_ylabel(\"Y-index\", fontsize=6)\n",
    "\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM//Z500_som_u_matrix_hit_map.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c3a9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten u-matrix & hit map\n",
    "U_flat = u_matrix.T.reshape(-1)  # back to (n_nodes,)\n",
    "hits_flat = hit_map.T.reshape(-1)  # back to (n_nodes,)\n",
    "\n",
    "# scale hits\n",
    "hits_scaled = 30 + 250 * (hits_flat / hits_flat.max())\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Scatter: U controls color, hits control bubble size\n",
    "sc = plt.scatter(\n",
    "    coords[:, 0],\n",
    "    coords[:, 1],\n",
    "    c=U_flat,\n",
    "    s=hits_scaled,\n",
    "    cmap=\"balance\",\n",
    "    edgecolor=\"k\",\n",
    "    linewidth=0.5,\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Draw lattice connections (right & down neighbors only)\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node = i * ydim + j\n",
    "\n",
    "        # right neighbor\n",
    "        if j + 1 < ydim:\n",
    "            nbr = i * ydim + (j + 1)\n",
    "            plt.plot(\n",
    "                [coords[node, 0], coords[nbr, 0]],\n",
    "                [coords[node, 1], coords[nbr, 1]],\n",
    "                \"k-\",\n",
    "                lw=0.6,\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "        # down neighbor\n",
    "        if i + 1 < xdim:\n",
    "            nbr = (i + 1) * ydim + j\n",
    "            plt.plot(\n",
    "                [coords[node, 0], coords[nbr, 0]],\n",
    "                [coords[node, 1], coords[nbr, 1]],\n",
    "                \"k-\",\n",
    "                lw=0.6,\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "# Node labels (i,j)\n",
    "for idx, (x, y) in enumerate(coords):\n",
    "    ix, iy = divmod(idx, ydim)\n",
    "    plt.text(x, y, f\"({ix},{iy})\", fontsize=8, ha=\"center\", va=\"center\", zorder=5)\n",
    "\n",
    "plt.title(\"Sammon / MDS Distortion Grid\\nU-Matrix (Color) \\\\& Node Frequency (Size)\")\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar(sc, label=\"U-Matrix (Avg. Neighbor Distance)\")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/Z500_som_sammon_mds.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3446c",
   "metadata": {},
   "source": [
    "### Node Weights Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5e6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ydim, xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600\n",
    ")\n",
    "\n",
    "# Shading levels for Z500\n",
    "levels_Z = np.arange(-1.4, 1.41, 0.2)\n",
    "\n",
    "# IVT contour levels\n",
    "levels_ivt = np.arange(-1.8, 1.81, 0.2)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # Fields for this node\n",
    "        Z_field = z500_nodes[i, j, :, :]\n",
    "        ivt_field = ivt_nodes[i, j, :, :]\n",
    "\n",
    "        # --- Z500 shaded ---\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            Z_field,\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        # --- IVT contours (black depending on preference) ---\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_field,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(f\"Node ({i},{j})\", fontsize=6)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# One shared colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_ticks(levels_Z)\n",
    "cbar.set_label(\"Standardized 500-hPa Anomaly\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\"Node Weight Patterns\", fontsize=8)\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/node_weights.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20f67b",
   "metadata": {},
   "source": [
    "### Anomaly Composite Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9de15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_days = pd.to_datetime(Z500_norm_daily.time.values).tz_localize(None)\n",
    "\n",
    "event_mask = np.isin(som_days.normalize(), pd.to_datetime(event_days))\n",
    "event_indices = np.where(event_mask)[0]\n",
    "\n",
    "# Create empty arrays for standardized anomalies\n",
    "z500_patterns = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ivt_patterns = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "\n",
    "counts = np.zeros((xdim, ydim), dtype=int)\n",
    "totals = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "        totals[i, j] = len(idx_node)\n",
    "\n",
    "        # Flash-flood days within this node\n",
    "        idx_event = np.intersect1d(idx_node, event_indices)\n",
    "        counts[i, j] = len(idx_event)\n",
    "\n",
    "        # Composite over *all* days in the node\n",
    "        if len(idx_node) > 0:\n",
    "            z500_patterns[i, j] = (\n",
    "                Z500_norm_daily.isel(time=idx_node).mean(\"time\").values\n",
    "            )\n",
    "            ivt_patterns[i, j] = (\n",
    "                IVT_norm_daily.isel(valid_time=idx_node).mean(\"valid_time\").values\n",
    "            )\n",
    "\n",
    "\n",
    "risk = np.zeros((xdim, ydim))\n",
    "risk[totals > 0] = counts[totals > 0] / totals[totals > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49553040",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ydim,\n",
    "    xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Levels for shading (Z500)\n",
    "levels_Z = np.arange(-2.0, 2.1, 0.25)\n",
    "\n",
    "# Fewer contour levels for ivt (to avoid clutter)\n",
    "levels_ivt = np.arange(-2.5, 2.6, 0.5)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # pull the Z500 & ivt composite fields for this node\n",
    "        Z_field = z500_patterns[i, j, :, :]\n",
    "        ivt_field = ivt_patterns[i, j, :, :]\n",
    "\n",
    "        # --- ivt shaded composite ---\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_field,\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"both\",\n",
    "        )\n",
    "\n",
    "        # --- z500 contour overlay ---\n",
    "        ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            Z_field,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(\n",
    "            f\"({i},{j})  FFE={counts[i, j]}/{totals[i, j]}  ({100 * risk[i, j]:.1f}\\\\%)\",\n",
    "            fontsize=5,\n",
    "        )\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# one colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"Standardized Anomaly\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"SOM Composite Anomalies: Z500 (contoured) + IVT (shaded)\", fontsize=8, y=1.04\n",
    ")\n",
    "plt.savefig(\n",
    "    \"figs/Z500-IVT-big-SOM/composite_anomalies.png\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86a127",
   "metadata": {},
   "source": [
    "### Composite Mean Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96757deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z500_patterns_raw = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ivt_patterns_raw = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "counts = np.zeros((xdim, ydim), dtype=int)\n",
    "totals = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "        totals[i, j] = len(idx_node)\n",
    "\n",
    "        # Flash-flood days within this node\n",
    "        idx_event = np.intersect1d(idx_node, event_indices)\n",
    "        counts[i, j] = len(idx_event)\n",
    "\n",
    "        # Composite over *all* days in the node\n",
    "        if len(idx_node) > 0:\n",
    "            z500_patterns_raw[i, j] = Z500_daily.isel(time=idx_node).mean(\"time\").values\n",
    "            ivt_patterns_raw[i, j] = (\n",
    "                IVT_daily.isel(valid_time=idx_node).mean(\"valid_time\").values\n",
    "            )\n",
    "\n",
    "risk = np.zeros((xdim, ydim))\n",
    "risk[totals > 0] = counts[totals > 0] / totals[totals > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d51c5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ydim,\n",
    "    xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Levels for shading (Z500)\n",
    "levels_Z = range(552, 595, 3)\n",
    "\n",
    "# IVT levels\n",
    "levels_ivt = np.arange(0, 701, 100)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # pull the Z500 & ivt composite fields for this node\n",
    "        Z_field = z500_patterns_raw[i, j, :, :]\n",
    "        ivt_field = ivt_patterns_raw[i, j, :, :]\n",
    "\n",
    "        # --- IVT shaded composite ---\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_field,\n",
    "            cmap=\"BuPu\",\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"max\"\n",
    "        )\n",
    "\n",
    "        # --- ivt contour overlay ---\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            Z_field / 98.1,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(\n",
    "            f\"({i},{j})  FFE={counts[i, j]}/{totals[i, j]}  ({100 * risk[i, j]:.1f}\\\\%)\",\n",
    "            fontsize=5,\n",
    "        )\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Add inline labels\n",
    "        ax.clabel(cn, cn.levels, fontsize=5)\n",
    "\n",
    "# one colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"IVT (kg m$^{-1}$ s$^{-1}$)\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\"SOM Composite: IVT (shaded) + Z500 (contoured)\", fontsize=8, y=1.04)\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/composite_mean_IVT_shaded.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e3a5f",
   "metadata": {},
   "source": [
    "### Maps of Individual Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90749094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 20 existing _FFE.png files\n"
     ]
    }
   ],
   "source": [
    "# Set number of columns\n",
    "cols = 4\n",
    "proj = ccrs.PlateCarree()\n",
    "\n",
    "# Clear out existing _FFE.png files in the indiv-nodes directory\n",
    "ffe_files = glob.glob(\"figs/Z500-IVT-big-SOM/indiv-nodes/*_FFE.png\")\n",
    "for file in ffe_files:\n",
    "    os.remove(file)\n",
    "print(f\"Removed {len(ffe_files)} existing _FFE.png files\")\n",
    "\n",
    "# Iterate through each node\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "\n",
    "        # Restrict to flash-flood days only\n",
    "        idx = np.intersect1d(idx_node, event_indices)\n",
    "        n = len(idx)\n",
    "\n",
    "        # Skip nodes with no flash-flood days\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        # Set number of rows\n",
    "        rows = int(np.ceil(n / cols))\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            rows,\n",
    "            cols,\n",
    "            figsize=(3 * cols, 2.5 * rows),\n",
    "            subplot_kw={\"projection\": proj},\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        # Ensure axes is always iterable\n",
    "        axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "        for k, ax in enumerate(axes):\n",
    "            if k < n:\n",
    "                t = idx[k]\n",
    "                z500_data = Z500_daily.isel(time=t)\n",
    "                ivt_data = IVT_daily.isel(valid_time=t)\n",
    "\n",
    "                cn = ax.contour(\n",
    "                    lon,\n",
    "                    lat,\n",
    "                    z500_data / 98.1,\n",
    "                    colors=\"black\",\n",
    "                    linewidths=0.5,\n",
    "                    levels=range(546, 595, 6),\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                )\n",
    "\n",
    "                im = ax.contourf(\n",
    "                    lon,\n",
    "                    lat,\n",
    "                    ivt_data,\n",
    "                    cmap=\"BuPu\",\n",
    "                    levels=levels_ivt,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    extend=\"max\",\n",
    "                )\n",
    "\n",
    "                ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "                ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "                ax.add_feature(cfeature.STATES, linewidth=0.2)\n",
    "\n",
    "                ax.set_title(\n",
    "                    pd.to_datetime(z500_data.time.values).strftime(\"%Y-%m-%d\"),\n",
    "                    fontsize=7,\n",
    "                )\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        fig.suptitle(\n",
    "            f\"Node ({i},{j})  Flash-Flood Days = {n}\",\n",
    "            fontsize=8,\n",
    "            y=1.02,\n",
    "        )\n",
    "\n",
    "        plt.savefig(\n",
    "            f\"figs/Z500-IVT-big-SOM/indiv-nodes/node_{i}_{j}_FFE.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kde6nj9qds",
   "metadata": {},
   "source": [
    "### Residual Analysis for Flash Flood Days\n",
    "\n",
    "Calculate residuals (observed - node centroid) for flash flood events to understand how individual events deviate from the typical pattern captured by each SOM node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2kpqdybhpng",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals, QE, and spatial correlation calculated for flash flood days:\n",
      "Flash flood counts per node:\n",
      "[[ 5  4  2  3  9]\n",
      " [ 2  1  3  1  5]\n",
      " [ 3  4  4  3  6]\n",
      " [ 8  8  6 10 28]]\n",
      "\n",
      "Mean QE per node:\n",
      "[[122.1 101.6 101.9  95.4 121.3]\n",
      " [115.   94.9  89.4 111.  104.3]\n",
      " [101.3  95.5  95.3  92.1 101.4]\n",
      " [120.1 106.9 111.4 112.2 126.7]]\n",
      "\n",
      "Mean spatial correlation (combined) per node:\n",
      "[[0.476 0.551 0.617 0.383 0.688]\n",
      " [0.484 0.269 0.093 0.284 0.544]\n",
      " [0.423 0.297 0.197 0.252 0.53 ]\n",
      " [0.528 0.504 0.495 0.423 0.655]]\n",
      "\n",
      "Mean Z500 correlation per node:\n",
      "[[ 0.54   0.592  0.831  0.376  0.736]\n",
      " [ 0.348  0.156  0.156  0.399  0.51 ]\n",
      " [ 0.243  0.097 -0.064  0.276  0.673]\n",
      " [ 0.603  0.588  0.604  0.44   0.729]]\n",
      "\n",
      "Mean IVT correlation per node:\n",
      "[[ 0.325  0.399  0.007  0.019  0.556]\n",
      " [ 0.399  0.233 -0.033  0.184  0.281]\n",
      " [ 0.341  0.171  0.435  0.17   0.38 ]\n",
      " [ 0.402  0.37   0.409  0.468  0.534]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate residuals, quantization error, and spatial correlation for flash flood days\n",
    "# Residual = observed (normalized) - node centroid weight\n",
    "# QE = Euclidean distance between observed vector and BMU weight (MiniSom definition)\n",
    "# Spatial correlation = Pearson correlation between observed and node weight patterns\n",
    "\n",
    "# Storage for mean residuals per node (only flash flood days)\n",
    "z500_residuals = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ivt_residuals = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ff_counts = np.zeros((xdim, ydim), dtype=int)\n",
    "mean_qe_per_node = np.full((xdim, ydim), np.nan)\n",
    "mean_corr_per_node = np.full((xdim, ydim), np.nan)  # Combined correlation\n",
    "mean_corr_z500_per_node = np.full((xdim, ydim), np.nan)\n",
    "mean_corr_ivt_per_node = np.full((xdim, ydim), np.nan)\n",
    "\n",
    "# Storage for per-event metrics (for ranking later)\n",
    "event_metrics_list = []  # Will store (date, node_i, node_j, qe, corr, corr_z500, corr_ivt)\n",
    "\n",
    "# Get SOM weights in original shape for distance calculation\n",
    "som_weights = som.get_weights()  # shape: (xdim, ydim, n_features)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "\n",
    "        # Flash-flood days within this node\n",
    "        idx_ff = np.intersect1d(idx_node, event_indices)\n",
    "        ff_counts[i, j] = len(idx_ff)\n",
    "\n",
    "        if len(idx_ff) > 0:\n",
    "            # Get observed normalized values for flash flood days\n",
    "            z500_obs = Z500_norm_daily.isel(time=idx_ff).values  # (n_ff, lat, lon)\n",
    "            ivt_obs = IVT_norm_daily.isel(valid_time=idx_ff).values  # (n_ff, lat, lon)\n",
    "\n",
    "            # Get node centroid (weights are already normalized)\n",
    "            z500_centroid = z500_nodes[i, j, :, :]  # (lat, lon)\n",
    "            ivt_centroid = ivt_nodes[i, j, :, :]  # (lat, lon)\n",
    "\n",
    "            # Calculate residuals: observed - centroid\n",
    "            z500_resid = z500_obs - z500_centroid[np.newaxis, :, :]\n",
    "            ivt_resid = ivt_obs - ivt_centroid[np.newaxis, :, :]\n",
    "\n",
    "            # Store mean residual for this node\n",
    "            z500_residuals[i, j] = np.mean(z500_resid, axis=0)\n",
    "            ivt_residuals[i, j] = np.mean(ivt_resid, axis=0)\n",
    "\n",
    "            # Calculate QE and spatial correlation for each flash flood event\n",
    "            qe_values = []\n",
    "            corr_values = []\n",
    "            corr_z500_values = []\n",
    "            corr_ivt_values = []\n",
    "\n",
    "            for k, t in enumerate(idx_ff):\n",
    "                # QE for single sample = distance to BMU weight\n",
    "                qe = np.linalg.norm(X[t] - som_weights[i, j])\n",
    "                qe_values.append(qe)\n",
    "\n",
    "                # Spatial correlation (Pearson) for combined vector\n",
    "                corr_combined = np.corrcoef(X[t], som_weights[i, j].flatten())[0, 1]\n",
    "                corr_values.append(corr_combined)\n",
    "\n",
    "                # Separate correlations for Z500 and IVT\n",
    "                z500_obs_flat = z500_obs[k].flatten()\n",
    "                ivt_obs_flat = ivt_obs[k].flatten()\n",
    "                z500_node_flat = z500_centroid.flatten()\n",
    "                ivt_node_flat = ivt_centroid.flatten()\n",
    "\n",
    "                corr_z500 = np.corrcoef(z500_obs_flat, z500_node_flat)[0, 1]\n",
    "                corr_ivt = np.corrcoef(ivt_obs_flat, ivt_node_flat)[0, 1]\n",
    "                corr_z500_values.append(corr_z500)\n",
    "                corr_ivt_values.append(corr_ivt)\n",
    "\n",
    "                # Store for ranking\n",
    "                event_date = pd.to_datetime(Z500_norm_daily.time.values[t])\n",
    "                event_metrics_list.append(\n",
    "                    (event_date, i, j, qe, corr_combined, corr_z500, corr_ivt)\n",
    "                )\n",
    "\n",
    "            mean_qe_per_node[i, j] = np.mean(qe_values)\n",
    "            mean_corr_per_node[i, j] = np.mean(corr_values)\n",
    "            mean_corr_z500_per_node[i, j] = np.mean(corr_z500_values)\n",
    "            mean_corr_ivt_per_node[i, j] = np.mean(corr_ivt_values)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "event_metrics_df = pd.DataFrame(\n",
    "    event_metrics_list,\n",
    "    columns=[\"date\", \"node_i\", \"node_j\", \"qe\", \"corr\", \"corr_z500\", \"corr_ivt\"],\n",
    ")\n",
    "\n",
    "# Sort by correlation (ascending = worst pattern match first)\n",
    "event_metrics_df_by_corr = event_metrics_df.sort_values(\"corr\", ascending=True)\n",
    "\n",
    "# Also keep QE-sorted version\n",
    "event_qe_df = event_metrics_df.sort_values(\"qe\", ascending=False)\n",
    "\n",
    "print(\"Residuals, QE, and spatial correlation calculated for flash flood days:\")\n",
    "print(f\"Flash flood counts per node:\\n{ff_counts.T}\")\n",
    "print(f\"\\nMean QE per node:\\n{np.round(mean_qe_per_node.T, 1)}\")\n",
    "print(f\"\\nMean spatial correlation (combined) per node:\\n{np.round(mean_corr_per_node.T, 3)}\")\n",
    "print(f\"\\nMean Z500 correlation per node:\\n{np.round(mean_corr_z500_per_node.T, 3)}\")\n",
    "print(f\"\\nMean IVT correlation per node:\\n{np.round(mean_corr_ivt_per_node.T, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "nwgro86l2mn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mean residuals for flash flood days (with mean QE and correlation in titles)\n",
    "fig, axes = plt.subplots(\n",
    "    ydim,\n",
    "    xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Symmetric levels for residuals (centered on zero)\n",
    "levels_z500_resid = np.arange(-2.0, 2.05, 0.2)\n",
    "levels_ivt_resid = np.arange(-2.0, 2.05, 0.2)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        z500_resid_field = z500_residuals[i, j, :, :]\n",
    "        ivt_resid_field = ivt_residuals[i, j, :, :]\n",
    "\n",
    "        # Skip nodes with no flash flood days\n",
    "        if np.isnan(z500_resid_field).all():\n",
    "            ax.set_title(f\"({i},{j})  n=0\", fontsize=5)\n",
    "            ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "            ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # IVT residuals shaded\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_resid_field,\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_ivt_resid,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"both\",\n",
    "        )\n",
    "\n",
    "        # Z500 residuals contoured\n",
    "        ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            z500_resid_field,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_z500_resid,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(\n",
    "            f\"({i},{j}) n={ff_counts[i, j]} r={mean_corr_per_node[i, j]:.2f}\",\n",
    "            fontsize=5,\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"Standardized Residual\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Mean Residuals (FFE days): Z500 (contoured) + IVT (shaded)\", fontsize=8, y=1.04\n",
    ")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/residuals_ffe.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "jr45wa2r9yl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Flood Events Ranked by Spatial Correlation (Worst Pattern Match First)\n",
      "=====================================================================================\n",
      "Rank  Date          Node      Corr    Z500_r  IVT_r   QE        \n",
      "-------------------------------------------------------------------------------------\n",
      "1     2012-06-22    (3,2)     -0.136   0.014   -0.155   96.6\n",
      "2     2004-07-02    (2,1)     0.027   -0.023   0.086   92.4\n",
      "3     2006-06-02    (2,2)     0.100   -0.447   0.530   88.7\n",
      "4     2013-05-09    (3,3)     0.108   0.205   0.209   102.6\n",
      "5     2000-09-03    (2,1)     0.125   0.232   -0.157   83.9\n",
      "6     2000-07-03    (2,1)     0.127   0.260   -0.027   91.8\n",
      "7     2017-08-02    (4,2)     0.128   0.687   -0.168   106.5\n",
      "8     2002-06-26    (1,2)     0.163   -0.021   0.102   94.8\n",
      "9     2006-07-21    (2,2)     0.175   -0.222   0.473   85.1\n",
      "10    2004-09-18    (2,2)     0.182   0.075   0.350   113.5\n",
      "11    2021-08-22    (0,1)     0.199   0.143   0.074   138.4\n",
      "12    2018-06-28    (1,2)     0.229   0.271   0.049   77.4\n",
      "13    2024-08-06    (2,3)     0.236   0.389   0.130   174.6\n",
      "14    2019-07-31    (1,2)     0.255   -0.431   0.208   117.4\n",
      "15    2011-08-19    (3,0)     0.255   0.204   0.012   83.8\n",
      "16    1998-06-13    (3,3)     0.255   0.022   0.419   128.3\n",
      "17    1997-07-15    (1,1)     0.269   0.156   0.233   94.9\n",
      "18    2006-06-29    (3,3)     0.278   0.437   0.355   118.0\n",
      "19    2012-08-01    (3,1)     0.284   0.399   0.184   111.0\n",
      "20    2022-07-18    (3,3)     0.310   0.238   0.365   116.9\n",
      "21    2011-08-14    (4,1)     0.311   0.626   0.065   85.9\n",
      "22    2013-05-08    (0,0)     0.313   0.233   0.070   116.1\n",
      "23    2020-07-10    (0,2)     0.313   0.083   0.276   116.6\n",
      "24    2005-07-06    (4,3)     0.320   0.508   0.485   113.5\n",
      "25    2018-08-11    (1,0)     0.323   0.314   0.158   100.1\n",
      "26    2012-07-18    (2,2)     0.329   0.339   0.386   93.9\n",
      "27    2004-09-28    (3,3)     0.332   0.150   0.577   120.5\n",
      "28    1998-08-17    (1,3)     0.343   0.515   0.138   98.3\n",
      "29    2018-07-17    (2,3)     0.357   0.421   0.370   81.8\n",
      "30    1996-09-08    (4,3)     0.363   0.395   0.447   94.7\n",
      "31    1999-08-26    (4,3)     0.393   0.438   0.220   104.2\n",
      "32    2021-06-08    (0,3)     0.408   0.322   0.213   147.9\n",
      "33    2015-05-31    (2,3)     0.409   0.301   0.580   113.9\n",
      "34    2002-09-02    (0,2)     0.411   0.276   0.611   97.5\n",
      "35    2007-07-18    (3,2)     0.423   0.337   0.387   82.9\n",
      "36    2001-06-17    (1,3)     0.424   0.418   0.221   103.9\n",
      "37    2010-10-11    (3,0)     0.431   0.524   -0.273   103.1\n",
      "38    2006-07-12    (1,3)     0.436   0.405   0.247   126.2\n",
      "39    2015-07-30    (0,3)     0.438   0.594   0.450   124.1\n",
      "40    1996-10-19    (0,0)     0.456   0.652   0.357   131.6\n",
      "41    2004-09-08    (3,3)     0.457   0.413   0.672   128.5\n",
      "42    2018-08-04    (0,3)     0.460   0.490   0.243   120.9\n",
      "43    2013-09-02    (4,2)     0.461   0.488   0.548   81.8\n",
      "44    2006-08-25    (3,0)     0.464   0.399   0.316   99.4\n",
      "45    2011-08-21    (3,2)     0.469   0.478   0.279   96.9\n",
      "46    2005-10-14    (0,0)     0.482   0.510   0.376   97.7\n",
      "47    2010-10-01    (4,3)     0.489   0.477   0.459   159.6\n",
      "48    2010-08-23    (0,0)     0.489   0.580   0.328   144.0\n",
      "49    2007-08-08    (1,3)     0.493   0.623   0.471   101.1\n",
      "50    2019-07-17    (1,3)     0.510   0.738   0.459   100.2\n",
      "51    2021-08-27    (0,3)     0.511   0.625   0.148   100.6\n",
      "52    1996-07-08    (4,1)     0.515   -0.168   0.087   140.0\n",
      "53    2008-06-14    (0,3)     0.521   0.610   0.508   137.9\n",
      "54    2012-08-15    (4,1)     0.531   0.396   0.356   79.5\n",
      "55    2001-08-13    (2,3)     0.541   0.788   0.275   92.8\n",
      "56    2016-07-25    (1,2)     0.542   0.568   0.324   92.3\n",
      "57    2019-07-22    (3,3)     0.545   0.572   0.442   80.9\n",
      "58    2006-06-01    (0,2)     0.546   0.372   0.137   89.9\n",
      "59    2013-06-03    (3,3)     0.554   0.781   0.470   93.5\n",
      "60    2000-08-11    (1,0)     0.555   0.641   0.304   102.8\n",
      "61    2018-08-07    (1,3)     0.562   0.406   0.566   88.5\n",
      "62    1996-07-31    (4,3)     0.572   0.738   0.549   116.1\n",
      "63    2021-07-02    (4,0)     0.575   0.841   0.505   146.9\n",
      "64    2006-08-10    (2,0)     0.575   0.830   -0.157   90.2\n",
      "65    2008-07-27    (4,0)     0.575   0.635   -0.045   90.3\n",
      "66    2014-08-31    (1,3)     0.581   0.722   0.579   140.0\n",
      "67    2019-08-07    (4,2)     0.584   0.612   0.231   107.9\n",
      "68    2022-09-13    (4,2)     0.590   0.693   0.667   86.7\n",
      "69    1999-09-16    (4,3)     0.595   0.607   0.652   168.2\n",
      "70    2012-09-08    (4,3)     0.610   0.622   0.436   113.8\n",
      "71    2007-06-27    (0,3)     0.613   0.553   0.523   96.2\n",
      "72    2021-07-08    (4,3)     0.614   0.823   0.722   97.7\n",
      "73    2003-09-23    (4,3)     0.618   0.672   0.380   114.0\n",
      "74    2017-05-05    (4,3)     0.630   0.523   0.731   147.5\n",
      "75    2002-08-16    (0,3)     0.631   0.827   0.594   128.5\n",
      "76    2007-10-11    (4,0)     0.632   0.801   0.501   84.1\n",
      "77    2003-08-04    (4,3)     0.637   0.752   0.681   113.1\n",
      "78    2012-06-06    (4,0)     0.639   0.795   0.580   124.9\n",
      "79    2014-07-02    (3,3)     0.640   0.813   0.348   118.4\n",
      "80    2014-10-22    (0,0)     0.640   0.724   0.491   120.9\n",
      "81    2018-09-25    (0,3)     0.644   0.806   0.538   104.6\n",
      "82    2000-08-28    (1,0)     0.648   0.724   0.637   97.9\n",
      "83    2008-08-15    (4,0)     0.651   0.645   0.662   112.4\n",
      "84    2014-05-17    (4,3)     0.652   0.914   0.239   153.1\n",
      "85    2004-06-25    (4,3)     0.657   0.649   0.473   127.1\n",
      "86    2003-08-17    (2,0)     0.658   0.833   0.172   113.6\n",
      "87    2014-06-13    (4,3)     0.660   0.783   0.364   95.4\n",
      "88    2014-05-10    (2,3)     0.664   0.821   0.644   105.3\n",
      "89    2014-07-03    (4,3)     0.671   0.864   0.683   125.8\n",
      "90    2021-09-01    (4,1)     0.679   0.856   0.453   116.9\n",
      "91    2004-06-17    (1,3)     0.679   0.877   0.280   97.3\n",
      "92    2011-08-01    (1,0)     0.680   0.689   0.496   105.4\n",
      "93    2007-07-11    (4,3)     0.684   0.613   0.596   136.5\n",
      "94    2019-07-23    (4,1)     0.685   0.840   0.441   99.2\n",
      "95    2008-06-22    (4,2)     0.689   0.871   0.415   78.5\n",
      "96    2023-07-16    (4,3)     0.707   0.720   0.605   122.0\n",
      "97    2018-07-27    (4,3)     0.707   0.892   0.416   91.2\n",
      "98    2017-08-04    (4,3)     0.714   0.864   0.324   123.1\n",
      "99    2006-10-28    (4,2)     0.730   0.686   0.590   146.7\n",
      "100   2004-08-11    (4,3)     0.733   0.835   0.305   157.2\n",
      "101   1996-07-13    (4,3)     0.733   0.816   0.668   119.4\n",
      "102   2008-08-11    (4,0)     0.734   0.728   0.545   117.9\n",
      "103   2007-06-04    (4,3)     0.743   0.792   0.493   143.9\n",
      "104   2013-05-23    (3,3)     0.747   0.771   0.818   114.1\n",
      "105   2018-09-28    (2,3)     0.762   0.905   0.453   99.9\n",
      "106   2009-07-26    (4,3)     0.762   0.802   0.554   97.9\n",
      "107   2023-09-29    (0,1)     0.769   0.554   0.724   91.5\n",
      "108   2015-07-15    (4,0)     0.780   0.678   0.783   131.7\n",
      "109   2017-06-19    (4,3)     0.788   0.868   0.664   159.5\n",
      "110   1996-07-03    (4,0)     0.794   0.811   0.622   135.9\n",
      "111   2008-09-06    (4,3)     0.795   0.865   0.728   154.7\n",
      "112   2008-08-14    (4,0)     0.809   0.692   0.852   147.4\n",
      "113   2003-07-22    (4,3)     0.810   0.876   0.645   117.3\n",
      "114   2014-07-15    (4,3)     0.836   0.812   0.777   137.8\n",
      "115   2009-07-29    (4,3)     0.845   0.894   0.661   142.2\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "Total flash flood events: 115\n",
      "Mean correlation (all FFE): 0.510\n",
      "Mean Z500 correlation: 0.543\n",
      "Mean IVT correlation: 0.394\n",
      "Mean QE (all FFE): 112.42\n"
     ]
    }
   ],
   "source": [
    "# Rank flash flood events by spatial correlation (worst pattern match = lowest correlation)\n",
    "print(\"Flash Flood Events Ranked by Spatial Correlation (Worst Pattern Match First)\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'Rank':<6}{'Date':<14}{'Node':<10}{'Corr':<8}{'Z500_r':<8}{'IVT_r':<8}{'QE':<10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for rank, (_, row) in enumerate(event_metrics_df_by_corr.iterrows(), start=1):\n",
    "    date_str = row[\"date\"].strftime(\"%Y-%m-%d\")\n",
    "    node_str = f\"({int(row['node_i'])},{int(row['node_j'])})\"\n",
    "    print(\n",
    "        f\"{rank:<6}{date_str:<14}{node_str:<10}{row['corr']:.3f}   \"\n",
    "        f\"{row['corr_z500']:.3f}   {row['corr_ivt']:.3f}   {row['qe']:.1f}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 85)\n",
    "print(f\"\\nTotal flash flood events: {len(event_metrics_df)}\")\n",
    "print(f\"Mean correlation (all FFE): {event_metrics_df['corr'].mean():.3f}\")\n",
    "print(f\"Mean Z500 correlation: {event_metrics_df['corr_z500'].mean():.3f}\")\n",
    "print(f\"Mean IVT correlation: {event_metrics_df['corr_ivt'].mean():.3f}\")\n",
    "print(f\"Mean QE (all FFE): {event_metrics_df['qe'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c4b4fq1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top N highest-QE flash flood events to visually inspect for cutoff lows\n",
    "n_top = 15\n",
    "top_events = event_qe_df.head(n_top)\n",
    "\n",
    "cols = 5\n",
    "rows = int(np.ceil(n_top / cols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    rows,\n",
    "    cols,\n",
    "    figsize=(3 * cols, 2.5 * rows),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    layout=\"constrained\",\n",
    "    dpi=300,\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Levels for Z500 contours (in dam) - extended range for cutoff lows\n",
    "levels_Z = range(534, 606, 3)\n",
    "levels_ivt = np.arange(0, 701, 100)\n",
    "\n",
    "for k, (_, row) in enumerate(top_events.iterrows()):\n",
    "    ax = axes[k]\n",
    "    \n",
    "    # Find the time index for this date\n",
    "    event_date = row[\"date\"]\n",
    "    t = np.where(pd.to_datetime(Z500_daily.time.values) == event_date)[0][0]\n",
    "    \n",
    "    z500_data = Z500_daily.isel(time=t)\n",
    "    ivt_data = IVT_daily.isel(valid_time=t)\n",
    "    \n",
    "    # IVT shaded\n",
    "    im = ax.contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        ivt_data,\n",
    "        cmap=\"BuPu\",\n",
    "        levels=levels_ivt,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        extend=\"max\",\n",
    "    )\n",
    "    \n",
    "    # Z500 contoured\n",
    "    cn = ax.contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        z500_data / 98.1,\n",
    "        colors=\"black\",\n",
    "        linewidths=0.6,\n",
    "        levels=levels_Z,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.3)\n",
    "    \n",
    "    ax.set_title(\n",
    "        f\"{event_date.strftime('%Y-%m-%d')}\\nNode ({int(row['node_i'])},{int(row['node_j'])})  QE={row['qe']:.1f}\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "# Turn off any unused axes\n",
    "for k in range(n_top, len(axes)):\n",
    "    axes[k].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Top {n_top} Highest-QE Flash Flood Events\", fontsize=10, y=1.02)\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/top_qe_events.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "oottt45lz9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bottom N lowest-QE flash flood events (best fit) for comparison\n",
    "n_bottom = 15\n",
    "bottom_events = event_qe_df.tail(n_bottom).iloc[::-1]  # Reverse so lowest QE is first\n",
    "\n",
    "cols = 5\n",
    "rows = int(np.ceil(n_bottom / cols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    rows,\n",
    "    cols,\n",
    "    figsize=(3 * cols, 2.5 * rows),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    layout=\"constrained\",\n",
    "    dpi=300,\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Levels for Z500 contours (in dam) - extended range\n",
    "levels_Z = range(534, 606, 3)\n",
    "levels_ivt = np.arange(0, 701, 100)\n",
    "\n",
    "for k, (_, row) in enumerate(bottom_events.iterrows()):\n",
    "    ax = axes[k]\n",
    "    \n",
    "    # Find the time index for this date\n",
    "    event_date = row[\"date\"]\n",
    "    t = np.where(pd.to_datetime(Z500_daily.time.values) == event_date)[0][0]\n",
    "    \n",
    "    z500_data = Z500_daily.isel(time=t)\n",
    "    ivt_data = IVT_daily.isel(valid_time=t)\n",
    "    \n",
    "    # IVT shaded\n",
    "    im = ax.contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        ivt_data,\n",
    "        cmap=\"BuPu\",\n",
    "        levels=levels_ivt,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        extend=\"max\",\n",
    "    )\n",
    "    \n",
    "    # Z500 contoured\n",
    "    cn = ax.contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        z500_data / 98.1,\n",
    "        colors=\"black\",\n",
    "        linewidths=0.6,\n",
    "        levels=levels_Z,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.3)\n",
    "    \n",
    "    ax.set_title(\n",
    "        f\"{event_date.strftime('%Y-%m-%d')}\\nNode ({int(row['node_i'])},{int(row['node_j'])})  QE={row['qe']:.1f}\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "# Turn off any unused axes\n",
    "for k in range(n_bottom, len(axes)):\n",
    "    axes[k].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Bottom {n_bottom} Lowest-QE Flash Flood Events (Best Fit)\", fontsize=10, y=1.02)\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/bottom_qe_events.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tm2h2u0ap3",
   "metadata": {},
   "source": [
    "### Cutoff Low Detection\n",
    "\n",
    "Simple automated detection of cutoff lows in flash flood events. A cutoff low is identified when there is a single local minimum in the Z500 field that is surrounded by higher values (i.e., enclosed by a closed contour using 6-dam spacing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ofcn1m97dj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff low detection results:\n",
      "Total flash flood events: 115\n",
      "Events with cutoff lows: 5 (4.3%)\n",
      "\n",
      "Cutoff fraction by node:\n",
      "[[60.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   7.1]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def has_cutoff_low(z500_field, contour_spacing=6):\n",
    "    \"\"\"\n",
    "    Detect if a Z500 field contains a cutoff low.\n",
    "\n",
    "    A cutoff low is identified as a local minimum where the contour\n",
    "    at (minimum + contour_spacing) forms a closed loop that does not\n",
    "    touch the domain boundary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z500_field : array-like\n",
    "        2D array of Z500 values in dam (decameters)\n",
    "    contour_spacing : float\n",
    "        Contour interval in dam. The closed contour is defined at\n",
    "        (local_minimum + contour_spacing).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if a cutoff low is detected, False otherwise\n",
    "    \"\"\"\n",
    "    z500 = np.asarray(z500_field)\n",
    "\n",
    "    # Find local minima using minimum filter\n",
    "    min_filtered = ndimage.minimum_filter(z500, size=5)\n",
    "    local_minima = z500 == min_filtered\n",
    "\n",
    "    # Get coordinates of local minima\n",
    "    min_coords = np.argwhere(local_minima)\n",
    "\n",
    "    for coord in min_coords:\n",
    "        i, j = coord\n",
    "        min_val = z500[i, j]\n",
    "\n",
    "        # Skip minima too close to the edge\n",
    "        if i < 2 or i >= z500.shape[0] - 2 or j < 2 or j >= z500.shape[1] - 2:\n",
    "            continue\n",
    "\n",
    "        # Define the closed contour level: minimum + contour_spacing\n",
    "        contour_level = min_val + contour_spacing\n",
    "\n",
    "        # Find the region enclosed by this contour (values < contour_level)\n",
    "        below_contour = z500 < contour_level\n",
    "\n",
    "        # Label connected regions\n",
    "        labeled, num_features = ndimage.label(below_contour)\n",
    "\n",
    "        if num_features == 0:\n",
    "            continue\n",
    "\n",
    "        # Get the label of the region containing this minimum\n",
    "        region_label = labeled[i, j]\n",
    "\n",
    "        if region_label == 0:\n",
    "            continue\n",
    "\n",
    "        # Check if this region touches any boundary\n",
    "        region_mask = labeled == region_label\n",
    "\n",
    "        touches_boundary = (\n",
    "            np.any(region_mask[0, :])  # top edge\n",
    "            or np.any(region_mask[-1, :])  # bottom edge\n",
    "            or np.any(region_mask[:, 0])  # left edge\n",
    "            or np.any(region_mask[:, -1])  # right edge\n",
    "        )\n",
    "\n",
    "        # If the region doesn't touch any boundary, it's a closed cutoff\n",
    "        if not touches_boundary:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Test on all flash flood events and track by node\n",
    "cutoff_by_node = {\n",
    "    (i, j): {\"total\": 0, \"cutoff\": 0} for i in range(xdim) for j in range(ydim)\n",
    "}\n",
    "cutoff_events = []  # Store (date, node_i, node_j, has_cutoff)\n",
    "\n",
    "for idx in event_indices:\n",
    "    # Get Z500 in dam\n",
    "    z500_dam = Z500_daily.isel(time=idx).values / 98.1\n",
    "\n",
    "    # Get BMU for this day\n",
    "    node_i, node_j = bmus[idx]\n",
    "\n",
    "    # Check for cutoff low\n",
    "    has_cutoff = has_cutoff_low(z500_dam, contour_spacing=6)\n",
    "\n",
    "    # Update counts\n",
    "    cutoff_by_node[(node_i, node_j)][\"total\"] += 1\n",
    "    if has_cutoff:\n",
    "        cutoff_by_node[(node_i, node_j)][\"cutoff\"] += 1\n",
    "\n",
    "    # Store event info\n",
    "    event_date = pd.to_datetime(Z500_daily.time.values[idx])\n",
    "    cutoff_events.append((event_date, node_i, node_j, has_cutoff))\n",
    "\n",
    "# Create arrays for visualization\n",
    "cutoff_fraction = np.full((xdim, ydim), np.nan)\n",
    "cutoff_count = np.zeros((xdim, ydim), dtype=int)\n",
    "total_ff_count = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        total = cutoff_by_node[(i, j)][\"total\"]\n",
    "        cutoff = cutoff_by_node[(i, j)][\"cutoff\"]\n",
    "        total_ff_count[i, j] = total\n",
    "        cutoff_count[i, j] = cutoff\n",
    "        if total > 0:\n",
    "            cutoff_fraction[i, j] = cutoff / total\n",
    "\n",
    "# Summary\n",
    "total_cutoffs = sum(1 for e in cutoff_events if e[3])\n",
    "print(f\"Cutoff low detection results:\")\n",
    "print(f\"Total flash flood events: {len(cutoff_events)}\")\n",
    "print(\n",
    "    f\"Events with cutoff lows: {total_cutoffs} ({100 * total_cutoffs / len(cutoff_events):.1f}%)\"\n",
    ")\n",
    "print(f\"\\nCutoff fraction by node:\")\n",
    "print(f\"{np.round(cutoff_fraction.T * 100, 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nynoadl4s8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cutoff low fraction by SOM node\n",
    "fig, ax = plt.subplots(figsize=(5, 4), dpi=150)\n",
    "\n",
    "# Plot cutoff fraction as heatmap\n",
    "im = ax.imshow(\n",
    "    cutoff_fraction.T * 100,\n",
    "    cmap=\"YlOrRd\",\n",
    "    origin=\"lower\",\n",
    "    vmin=0,\n",
    "    vmax=100,\n",
    ")\n",
    "\n",
    "# Add text annotations with fraction and counts\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        total = total_ff_count[i, j]\n",
    "        cutoff = cutoff_count[i, j]\n",
    "        frac = cutoff_fraction[i, j]\n",
    "        \n",
    "        if total > 0:\n",
    "            text = f\"{100 * frac:.0f}\\\\%\\n({cutoff}/{total})\"\n",
    "        else:\n",
    "            text = \"n=0\"\n",
    "        \n",
    "        # Choose text color based on background\n",
    "        text_color = \"white\" if frac > 0.5 else \"black\"\n",
    "        ax.text(i, j, text, ha=\"center\", va=\"center\", fontsize=7, color=text_color)\n",
    "\n",
    "ax.set_xticks(np.arange(xdim))\n",
    "ax.set_yticks(np.arange(ydim))\n",
    "ax.set_xlabel(\"X-index\", fontsize=8)\n",
    "ax.set_ylabel(\"Y-index\", fontsize=8)\n",
    "ax.set_title(\"Cutoff Low Frequency in Flash Flood Events by SOM Node\", fontsize=9)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Cutoff Low Frequency (\\\\%)\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/cutoff_low_frequency.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a95e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soms314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
