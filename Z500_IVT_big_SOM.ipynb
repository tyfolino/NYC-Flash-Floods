{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4692b8",
   "metadata": {},
   "source": [
    "# All Days SOM Training (Z<sub>500</sub> and |IVT|)\n",
    "\n",
    "By: Ty Janoski\n",
    "\n",
    "Updated 1/24/2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efd764",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb4540",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7986ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmweather  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots  # noqa: F401\n",
    "import xarray as xr\n",
    "from minisom import MiniSom\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "plt.style.use([\"science\", \"nature\", \"grid\"])\n",
    "plt.rcParams[\"text.usetex\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a515db",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7b46a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Z500 at flash-flood event times\n",
    "path = \"/mnt/drive2/SOM_intermediate_files/\"\n",
    "\n",
    "# Z500\n",
    "Z500_daily = xr.load_dataarray(f\"{path}era5_Z500_daily.nc\")\n",
    "Z500_norm_daily = xr.load_dataarray(f\"{path}era5_Z500_norm_daily.nc\")\n",
    "Z500_norm_weighted_daily = xr.load_dataarray(f\"{path}era5_Z500_norm_weighted_daily.nc\")\n",
    "\n",
    "# IVT\n",
    "IVT_daily = xr.load_dataset(f\"{path}era5_ivt_daily.nc\")[\"ivt\"]\n",
    "IVT_norm_daily = xr.load_dataset(f\"{path}era5_ivt_norm_daily.nc\")[\"ivt\"]\n",
    "IVT_norm_weighted_daily = xr.load_dataset(f\"{path}era5_ivt_norm_weighted_daily.nc\")[\n",
    "    \"ivt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5206a1",
   "metadata": {},
   "source": [
    "### Reshape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd7b2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data for SOM training\n",
    "Z500_flat = Z500_norm_weighted_daily.stack(\n",
    "    features=[\"lat\", \"lon\"]\n",
    ").values  # shape: (time, space)\n",
    "IVT_flat = IVT_norm_weighted_daily.stack(\n",
    "    features=[\"latitude\", \"longitude\"]\n",
    ").values  # shape: (time, space)\n",
    "\n",
    "X = np.concatenate([Z500_flat, IVT_flat], axis=1)  # shape: (time, space*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b80c4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read flash flood events and filter to unique episodes\n",
    "df = pd.read_csv(\"data/storm_data_search_results.csv\")\n",
    "df = df[df[\"EVENT_ID\"].astype(str).str.isdigit()].drop_duplicates(\n",
    "    subset=[\"EPISODE_ID\"], keep=\"first\"\n",
    ")\n",
    "\n",
    "# Parse begin datetime: combine date and time, convert to UTC\n",
    "df[\"BEGIN_DATETIME\"] = (\n",
    "    pd.to_datetime(\n",
    "        df[\"BEGIN_DATE\"]\n",
    "        + \" \"\n",
    "        + df[\"BEGIN_TIME\"].fillna(0).astype(int).astype(str).str.zfill(4),\n",
    "        format=\"%m/%d/%Y %H%M\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    .dt.tz_localize(\"EST\", ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "    .dt.tz_convert(\"UTC\")\n",
    ")\n",
    "\n",
    "# Extract unique event days (timezone-naive for xarray compatibility)\n",
    "event_days = sorted(df[\"BEGIN_DATETIME\"].dt.floor(\"D\").dt.tz_localize(None).unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44096767",
   "metadata": {},
   "source": [
    "## SOM Training\n",
    "\n",
    "We are going to train our SOM with random initialization and online training. We will also use two phases: a \"coarse\" phase with a larger sigma and learning rate, then a \"fine\" phase with a smaller learning rate and sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1c099",
   "metadata": {},
   "source": [
    "### Set SOM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c57aa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SOM shape\n",
    "xdim, ydim = 5, 4\n",
    "\n",
    "# Set number of iterations for each phase\n",
    "n1, n2 = 5000, 5000\n",
    "\n",
    "# Set starting sigmas\n",
    "sig1, sig2 = np.sqrt(xdim**2 + ydim**2), 1.0\n",
    "\n",
    "# Set starting learning rates\n",
    "lr1, lr2 = 0.3, 0.1\n",
    "\n",
    "# Random seed for reproducibility\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd0c59",
   "metadata": {},
   "source": [
    "### Train SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83fe49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 5000 / 5000 ] 100% - 0:00:00 left \n",
      " quantization error: 116.90000597047171\n",
      "0.0007496251874062968\n",
      " [ 5000 / 5000 ] 100% - 0:00:00 left \n",
      " quantization error: 114.15675919096053\n",
      "0.0031859070464767617\n"
     ]
    }
   ],
   "source": [
    "# Create SOM instance\n",
    "som = MiniSom(\n",
    "    xdim,\n",
    "    ydim,\n",
    "    input_len=X.shape[1],\n",
    "    sigma=sig1,\n",
    "    learning_rate=lr1,\n",
    "    decay_function=\"linear_decay_to_zero\",\n",
    "    sigma_decay_function=\"linear_decay_to_one\",\n",
    "    neighborhood_function=\"gaussian\",\n",
    "    random_seed=random_seed,\n",
    ")\n",
    "\n",
    "# Initialize random weights\n",
    "som.random_weights_init(X)\n",
    "\n",
    "# Random training\n",
    "som.train_random(X, n1, verbose=True)\n",
    "print(som.topographic_error(X))\n",
    "\n",
    "# Phase 2\n",
    "som._sigma = sig2  # type: ignore\n",
    "som._learning_rate = lr2\n",
    "som.train_random(X, n2, verbose=True)\n",
    "print(som.topographic_error(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f85390",
   "metadata": {},
   "source": [
    "### Grab important fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5e5a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janoski/miniforge3/envs/soms314/lib/python3.14/site-packages/sklearn/manifold/_mds.py:754: FutureWarning: The default value of `init` will change from 'random' to 'classical_mds' in 1.10. To suppress this warning, provide some value of `init`.\n",
      "  warnings.warn(\n",
      "/home/janoski/miniforge3/envs/soms314/lib/python3.14/site-packages/sklearn/manifold/_mds.py:771: FutureWarning: The `dissimilarity` parameter is deprecated and will be removed in 1.10. Use `metric` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Total node number\n",
    "n_nodes = xdim * ydim\n",
    "\n",
    "# Get flattened weights\n",
    "weights = som.get_weights().reshape(xdim * ydim, -1)\n",
    "\n",
    "# u-matrix\n",
    "u_matrix = som.distance_map().T\n",
    "\n",
    "# bmus & hit_map\n",
    "bmus = np.array([som.winner(x) for x in X])\n",
    "\n",
    "hit_map = np.zeros((xdim, ydim))\n",
    "for i, j in bmus:\n",
    "    hit_map[i, j] += 1\n",
    "hit_map = hit_map.T\n",
    "\n",
    "# Sammon Coordinates\n",
    "D = pairwise_distances(weights)\n",
    "coords = MDS(\n",
    "    n_components=2, dissimilarity=\"precomputed\", random_state=42, n_init=4\n",
    ").fit_transform(D)\n",
    "\n",
    "# Get lats/lons\n",
    "lat = Z500_norm_weighted_daily.lat\n",
    "lon = Z500_norm_weighted_daily.lon\n",
    "\n",
    "# Dimensions of the spatial field\n",
    "n_lat = lat.size\n",
    "n_lon = lon.size\n",
    "n_features = n_lat * n_lon\n",
    "\n",
    "# Split weights into Z500 and IVT components\n",
    "z500_weights = weights[:, :n_features]\n",
    "ivt_weights = weights[:, n_features:]\n",
    "\n",
    "# Reshape weights back to spatial dimensions\n",
    "z500_nodes = z500_weights.reshape(xdim, ydim, n_lat, n_lon)\n",
    "ivt_nodes = ivt_weights.reshape(xdim, ydim, n_lat, n_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666d0b8",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6a70f",
   "metadata": {},
   "source": [
    "### U-matrix and Sammon Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8563653",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, layout=\"constrained\", figsize=(6, 3), dpi=600)\n",
    "\n",
    "# u-matrix\n",
    "im0 = axes[0].imshow(u_matrix, cmap=\"viridis\", origin=\"lower\")\n",
    "axes[0].set_title(\"U-Matrix (Mean Inter-Node Distance)\", fontsize=7)\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04, shrink=0.7)\n",
    "\n",
    "# hit map\n",
    "im1 = axes[1].imshow(hit_map, cmap=\"plasma\", origin=\"lower\")\n",
    "axes[1].set_title(\"Hit Map (Samples per Node)\", fontsize=7)\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04, shrink=0.7)\n",
    "\n",
    "# axis styling\n",
    "for ax in axes:\n",
    "    ax.set_xticks(np.arange(xdim))\n",
    "    ax.set_yticks(np.arange(ydim))\n",
    "    ax.set_xlabel(\"X-index\", fontsize=6)\n",
    "    ax.set_ylabel(\"Y-index\", fontsize=6)\n",
    "\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM//Z500_som_u_matrix_hit_map.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c3a9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten u-matrix & hit map\n",
    "U_flat = u_matrix.T.reshape(-1)  # back to (n_nodes,)\n",
    "hits_flat = hit_map.T.reshape(-1)  # back to (n_nodes,)\n",
    "\n",
    "# scale hits\n",
    "hits_scaled = 30 + 250 * (hits_flat / hits_flat.max())\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Scatter: U controls color, hits control bubble size\n",
    "sc = plt.scatter(\n",
    "    coords[:, 0],\n",
    "    coords[:, 1],\n",
    "    c=U_flat,\n",
    "    s=hits_scaled,\n",
    "    cmap=\"balance\",\n",
    "    edgecolor=\"k\",\n",
    "    linewidth=0.5,\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Draw lattice connections (right & down neighbors only)\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node = i * ydim + j\n",
    "\n",
    "        # right neighbor\n",
    "        if j + 1 < ydim:\n",
    "            nbr = i * ydim + (j + 1)\n",
    "            plt.plot(\n",
    "                [coords[node, 0], coords[nbr, 0]],\n",
    "                [coords[node, 1], coords[nbr, 1]],\n",
    "                \"k-\",\n",
    "                lw=0.6,\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "        # down neighbor\n",
    "        if i + 1 < xdim:\n",
    "            nbr = (i + 1) * ydim + j\n",
    "            plt.plot(\n",
    "                [coords[node, 0], coords[nbr, 0]],\n",
    "                [coords[node, 1], coords[nbr, 1]],\n",
    "                \"k-\",\n",
    "                lw=0.6,\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "# Node labels (i,j)\n",
    "for idx, (x, y) in enumerate(coords):\n",
    "    ix, iy = divmod(idx, ydim)\n",
    "    plt.text(x, y, f\"({ix},{iy})\", fontsize=8, ha=\"center\", va=\"center\", zorder=5)\n",
    "\n",
    "plt.title(\"Sammon / MDS Distortion Grid\\nU-Matrix (Color) \\\\& Node Frequency (Size)\")\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar(sc, label=\"U-Matrix (Avg. Neighbor Distance)\")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/Z500_som_sammon_mds.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3446c",
   "metadata": {},
   "source": [
    "### Node Weights Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa5e6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ydim, xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600\n",
    ")\n",
    "\n",
    "# Shading levels for Z500\n",
    "levels_Z = np.arange(-1.4, 1.41, 0.2)\n",
    "\n",
    "# IVT contour levels\n",
    "levels_ivt = np.arange(-1.8, 1.81, 0.2)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # Fields for this node\n",
    "        Z_field = z500_nodes[i, j, :, :]\n",
    "        ivt_field = ivt_nodes[i, j, :, :]\n",
    "\n",
    "        # --- Z500 shaded ---\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            Z_field,\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        # --- IVT contours (black depending on preference) ---\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_field,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(f\"Node ({i},{j})\", fontsize=6)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# One shared colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_ticks(levels_Z)\n",
    "cbar.set_label(\"Standardized 500-hPa Anomaly\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\"All Days SOM: Node Weight Patterns\", fontsize=8)\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/node_weights.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20f67b",
   "metadata": {},
   "source": [
    "### Anomaly Composite Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c9de15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_days = pd.to_datetime(Z500_norm_daily.time.values).tz_localize(None)\n",
    "\n",
    "event_mask = np.isin(som_days.normalize(), pd.to_datetime(event_days))\n",
    "event_indices = np.where(event_mask)[0]\n",
    "\n",
    "# Create empty arrays for standardized anomalies\n",
    "z500_patterns = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ivt_patterns = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "\n",
    "counts = np.zeros((xdim, ydim), dtype=int)\n",
    "totals = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "        totals[i, j] = len(idx_node)\n",
    "\n",
    "        # Flash-flood days within this node\n",
    "        idx_event = np.intersect1d(idx_node, event_indices)\n",
    "        counts[i, j] = len(idx_event)\n",
    "\n",
    "        # Composite over *all* days in the node\n",
    "        if len(idx_node) > 0:\n",
    "            z500_patterns[i, j] = (\n",
    "                Z500_norm_daily.isel(time=idx_node).mean(\"time\").values\n",
    "            )\n",
    "            ivt_patterns[i, j] = (\n",
    "                IVT_norm_daily.isel(valid_time=idx_node).mean(\"valid_time\").values\n",
    "            )\n",
    "\n",
    "\n",
    "risk = np.zeros((xdim, ydim))\n",
    "risk[totals > 0] = counts[totals > 0] / totals[totals > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9212bygdbx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 114 FFE BMU assignments to data/som_5x4_ffe_bmus.csv\n"
     ]
    }
   ],
   "source": [
    "# Save BMUs for flash flood event days for cross-SOM analysis\n",
    "ffe_timestamps = pd.to_datetime(Z500_norm_daily.time.values[event_indices])\n",
    "ffe_bmus = bmus[event_indices]\n",
    "\n",
    "bmu_df_big = pd.DataFrame({\n",
    "    \"timestamp\": ffe_timestamps,\n",
    "    \"node_i\": ffe_bmus[:, 0],\n",
    "    \"node_j\": ffe_bmus[:, 1],\n",
    "})\n",
    "bmu_df_big.to_csv(\"data/som_5x4_ffe_bmus.csv\", index=False)\n",
    "print(f\"Saved {len(bmu_df_big)} FFE BMU assignments to data/som_5x4_ffe_bmus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49553040",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ydim,\n",
    "    xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Levels for shading (Z500)\n",
    "levels_Z = np.arange(-2.0, 2.1, 0.25)\n",
    "\n",
    "# Fewer contour levels for ivt (to avoid clutter)\n",
    "levels_ivt = np.arange(-2.5, 2.6, 0.5)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # pull the Z500 & ivt composite fields for this node\n",
    "        Z_field = z500_patterns[i, j, :, :]\n",
    "        ivt_field = ivt_patterns[i, j, :, :]\n",
    "\n",
    "        # --- ivt shaded composite ---\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_field,\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"both\",\n",
    "        )\n",
    "\n",
    "        # --- z500 contour overlay ---\n",
    "        ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            Z_field,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(\n",
    "            f\"({i},{j})  FFE={counts[i, j]}/{totals[i, j]}  ({100 * risk[i, j]:.1f}\\\\%)\",\n",
    "            fontsize=5,\n",
    "        )\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# one colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"Standardized Anomaly\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"All Days SOM Composite Anomalies: Z500 (contoured) + IVT (shaded)\",\n",
    "    fontsize=8,\n",
    "    y=1.04,\n",
    ")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/composite_anomalies.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86a127",
   "metadata": {},
   "source": [
    "### Composite Mean Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96757deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z500_patterns_raw = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ivt_patterns_raw = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "counts = np.zeros((xdim, ydim), dtype=int)\n",
    "totals = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "        totals[i, j] = len(idx_node)\n",
    "\n",
    "        # Flash-flood days within this node\n",
    "        idx_event = np.intersect1d(idx_node, event_indices)\n",
    "        counts[i, j] = len(idx_event)\n",
    "\n",
    "        # Composite over *all* days in the node\n",
    "        if len(idx_node) > 0:\n",
    "            z500_patterns_raw[i, j] = Z500_daily.isel(time=idx_node).mean(\"time\").values\n",
    "            ivt_patterns_raw[i, j] = (\n",
    "                IVT_daily.isel(valid_time=idx_node).mean(\"valid_time\").values\n",
    "            )\n",
    "\n",
    "risk = np.zeros((xdim, ydim))\n",
    "risk[totals > 0] = counts[totals > 0] / totals[totals > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d51c5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    ydim,\n",
    "    xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Levels for shading (Z500)\n",
    "levels_Z = range(552, 595, 3)\n",
    "\n",
    "# IVT levels\n",
    "levels_ivt = np.arange(0, 701, 100)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # pull the Z500 & ivt composite fields for this node\n",
    "        Z_field = z500_patterns_raw[i, j, :, :]\n",
    "        ivt_field = ivt_patterns_raw[i, j, :, :]\n",
    "\n",
    "        # --- IVT shaded composite ---\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_field,\n",
    "            cmap=\"BuPu\",\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"max\",\n",
    "        )\n",
    "\n",
    "        # --- ivt contour overlay ---\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            Z_field / 98.1,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(\n",
    "            f\"({i},{j})  FFE={counts[i, j]}/{totals[i, j]}  ({100 * risk[i, j]:.1f}\\\\%)\",\n",
    "            fontsize=5,\n",
    "        )\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Add inline labels\n",
    "        ax.clabel(cn, cn.levels, fontsize=5)\n",
    "\n",
    "# one colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"IVT (kg m$^{-1}$ s$^{-1}$)\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"All Days SOM Composite: IVT (shaded) + Z500 (contoured)\", fontsize=8, y=1.04\n",
    ")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/composite_mean_IVT_shaded.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e3a5f",
   "metadata": {},
   "source": [
    "### Maps of Individual Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90749094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 20 existing _FFE.png files\n"
     ]
    }
   ],
   "source": [
    "# Set number of columns\n",
    "cols = 5\n",
    "proj = ccrs.PlateCarree()\n",
    "\n",
    "# Clear out existing _FFE.png files in the indiv-nodes directory\n",
    "ffe_files = glob.glob(\"figs/Z500-IVT-big-SOM/indiv-nodes/*_FFE.png\")\n",
    "for file in ffe_files:\n",
    "    os.remove(file)\n",
    "print(f\"Removed {len(ffe_files)} existing _FFE.png files\")\n",
    "\n",
    "# Iterate through each node\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "\n",
    "        # Restrict to flash-flood days only\n",
    "        idx = np.intersect1d(idx_node, event_indices)\n",
    "        n = len(idx)\n",
    "\n",
    "        # Skip nodes with no flash-flood days\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        # Set number of rows\n",
    "        rows = int(np.ceil(n / cols))\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            rows,\n",
    "            cols,\n",
    "            figsize=(3 * cols, 2.5 * rows),\n",
    "            subplot_kw={\"projection\": proj},\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        # Ensure axes is always iterable\n",
    "        axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "        for k, ax in enumerate(axes):\n",
    "            if k < n:\n",
    "                t = idx[k]\n",
    "                z500_data = Z500_daily.isel(time=t)\n",
    "                ivt_data = IVT_daily.isel(valid_time=t)\n",
    "\n",
    "                cn = ax.contour(\n",
    "                    lon,\n",
    "                    lat,\n",
    "                    z500_data / 98.1,\n",
    "                    colors=\"black\",\n",
    "                    linewidths=0.5,\n",
    "                    levels=range(546, 595, 6),\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                )\n",
    "\n",
    "                im = ax.contourf(\n",
    "                    lon,\n",
    "                    lat,\n",
    "                    ivt_data,\n",
    "                    cmap=\"BuPu\",\n",
    "                    levels=levels_ivt,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    extend=\"max\",\n",
    "                )\n",
    "\n",
    "                ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "                ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "                ax.add_feature(cfeature.STATES, linewidth=0.2)\n",
    "\n",
    "                ax.set_title(\n",
    "                    pd.to_datetime(z500_data.time.values).strftime(\"%Y-%m-%d\"),\n",
    "                    fontsize=7,\n",
    "                )\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        fig.suptitle(\n",
    "            f\"Node ({i},{j})  Flash-Flood Days = {n}\",\n",
    "            fontsize=8,\n",
    "            y=1.02,\n",
    "        )\n",
    "\n",
    "        plt.savefig(\n",
    "            f\"figs/Z500-IVT-big-SOM/indiv-nodes/node_{i}_{j}_FFE.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kde6nj9qds",
   "metadata": {},
   "source": [
    "### Residual Analysis for Flash Flood Days\n",
    "\n",
    "Calculate residuals (observed - node centroid) for flash flood events to understand how individual events deviate from the typical pattern captured by each SOM node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2kpqdybhpng",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals, QE, and spatial correlation calculated for flash flood days:\n",
      "Flash flood counts per node:\n",
      "[[ 6  4  9 15 22]\n",
      " [ 5  5  6  3  5]\n",
      " [ 3  1  2  3  4]\n",
      " [ 5  2  3  2  9]]\n",
      "\n",
      "Mean QE per node:\n",
      "[[113.1  98.6 102.3 122.2 122.8]\n",
      " [115.3  92.6 112.1  97.3 109.1]\n",
      " [114.4  83.5  89.4 116.9 105. ]\n",
      " [118.8 118.2 100.3 120.9 111.9]]\n",
      "\n",
      "Mean spatial correlation (combined) per node:\n",
      "[[0.645 0.543 0.486 0.511 0.686]\n",
      " [0.46  0.318 0.252 0.023 0.596]\n",
      " [0.358 0.274 0.137 0.278 0.437]\n",
      " [0.401 0.586 0.503 0.475 0.664]]\n",
      "\n",
      "Mean Z500 correlation per node:\n",
      "[[ 0.71   0.691  0.515  0.602  0.746]\n",
      " [ 0.587  0.222  0.154  0.127  0.555]\n",
      " [ 0.077 -0.077  0.029  0.422  0.353]\n",
      " [ 0.387  0.593  0.567  0.617  0.729]]\n",
      "\n",
      "Mean IVT correlation per node:\n",
      "[[ 0.585  0.446  0.49   0.498  0.48 ]\n",
      " [ 0.284  0.256  0.393 -0.016  0.389]\n",
      " [ 0.433  0.407 -0.106  0.159  0.295]\n",
      " [ 0.206  0.458  0.343  0.169  0.443]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate residuals, quantization error, and spatial correlation for flash flood days\n",
    "# Residual = observed (normalized) - node centroid weight\n",
    "# QE = Euclidean distance between observed vector and BMU weight (MiniSom definition)\n",
    "# Spatial correlation = Pearson correlation between observed and node weight patterns\n",
    "\n",
    "# Storage for mean residuals per node (only flash flood days)\n",
    "z500_residuals = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ivt_residuals = np.full((xdim, ydim, n_lat, n_lon), np.nan)\n",
    "ff_counts = np.zeros((xdim, ydim), dtype=int)\n",
    "mean_qe_per_node = np.full((xdim, ydim), np.nan)\n",
    "mean_corr_per_node = np.full((xdim, ydim), np.nan)  # Combined correlation\n",
    "mean_corr_z500_per_node = np.full((xdim, ydim), np.nan)\n",
    "mean_corr_ivt_per_node = np.full((xdim, ydim), np.nan)\n",
    "\n",
    "# Storage for per-event metrics (for ranking later)\n",
    "event_metrics_list = []  # Will store (date, node_i, node_j, qe, corr, corr_z500, corr_ivt)\n",
    "\n",
    "# Get SOM weights in original shape for distance calculation\n",
    "som_weights = som.get_weights()  # shape: (xdim, ydim, n_features)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        # All days assigned to this node\n",
    "        idx_node = np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "\n",
    "        # Flash-flood days within this node\n",
    "        idx_ff = np.intersect1d(idx_node, event_indices)\n",
    "        ff_counts[i, j] = len(idx_ff)\n",
    "\n",
    "        if len(idx_ff) > 0:\n",
    "            # Get observed normalized values for flash flood days\n",
    "            z500_obs = Z500_norm_daily.isel(time=idx_ff).values  # (n_ff, lat, lon)\n",
    "            ivt_obs = IVT_norm_daily.isel(valid_time=idx_ff).values  # (n_ff, lat, lon)\n",
    "\n",
    "            # Get node centroid (weights are already normalized)\n",
    "            z500_centroid = z500_nodes[i, j, :, :]  # (lat, lon)\n",
    "            ivt_centroid = ivt_nodes[i, j, :, :]  # (lat, lon)\n",
    "\n",
    "            # Calculate residuals: observed - centroid\n",
    "            z500_resid = z500_obs - z500_centroid[np.newaxis, :, :]\n",
    "            ivt_resid = ivt_obs - ivt_centroid[np.newaxis, :, :]\n",
    "\n",
    "            # Store mean residual for this node\n",
    "            z500_residuals[i, j] = np.mean(z500_resid, axis=0)\n",
    "            ivt_residuals[i, j] = np.mean(ivt_resid, axis=0)\n",
    "\n",
    "            # Calculate QE and spatial correlation for each flash flood event\n",
    "            qe_values = []\n",
    "            corr_values = []\n",
    "            corr_z500_values = []\n",
    "            corr_ivt_values = []\n",
    "\n",
    "            for k, t in enumerate(idx_ff):\n",
    "                # QE for single sample = distance to BMU weight\n",
    "                qe = np.linalg.norm(X[t] - som_weights[i, j])\n",
    "                qe_values.append(qe)\n",
    "\n",
    "                # Spatial correlation (Pearson) for combined vector\n",
    "                corr_combined = np.corrcoef(X[t], som_weights[i, j].flatten())[0, 1]\n",
    "                corr_values.append(corr_combined)\n",
    "\n",
    "                # Separate correlations for Z500 and IVT\n",
    "                z500_obs_flat = z500_obs[k].flatten()\n",
    "                ivt_obs_flat = ivt_obs[k].flatten()\n",
    "                z500_node_flat = z500_centroid.flatten()\n",
    "                ivt_node_flat = ivt_centroid.flatten()\n",
    "\n",
    "                corr_z500 = np.corrcoef(z500_obs_flat, z500_node_flat)[0, 1]\n",
    "                corr_ivt = np.corrcoef(ivt_obs_flat, ivt_node_flat)[0, 1]\n",
    "                corr_z500_values.append(corr_z500)\n",
    "                corr_ivt_values.append(corr_ivt)\n",
    "\n",
    "                # Store for ranking\n",
    "                event_date = pd.to_datetime(Z500_norm_daily.time.values[t])\n",
    "                event_metrics_list.append(\n",
    "                    (event_date, i, j, qe, corr_combined, corr_z500, corr_ivt)\n",
    "                )\n",
    "\n",
    "            mean_qe_per_node[i, j] = np.mean(qe_values)\n",
    "            mean_corr_per_node[i, j] = np.mean(corr_values)\n",
    "            mean_corr_z500_per_node[i, j] = np.mean(corr_z500_values)\n",
    "            mean_corr_ivt_per_node[i, j] = np.mean(corr_ivt_values)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "event_metrics_df = pd.DataFrame(\n",
    "    event_metrics_list,\n",
    "    columns=[\"date\", \"node_i\", \"node_j\", \"qe\", \"corr\", \"corr_z500\", \"corr_ivt\"],\n",
    ")\n",
    "\n",
    "# Sort by correlation (ascending = worst pattern match first)\n",
    "event_metrics_df_by_corr = event_metrics_df.sort_values(\"corr\", ascending=True)\n",
    "\n",
    "# Also keep QE-sorted version\n",
    "event_qe_df = event_metrics_df.sort_values(\"qe\", ascending=False)\n",
    "\n",
    "print(\"Residuals, QE, and spatial correlation calculated for flash flood days:\")\n",
    "print(f\"Flash flood counts per node:\\n{ff_counts.T}\")\n",
    "print(f\"\\nMean QE per node:\\n{np.round(mean_qe_per_node.T, 1)}\")\n",
    "print(\n",
    "    f\"\\nMean spatial correlation (combined) per node:\\n{np.round(mean_corr_per_node.T, 3)}\"\n",
    ")\n",
    "print(f\"\\nMean Z500 correlation per node:\\n{np.round(mean_corr_z500_per_node.T, 3)}\")\n",
    "print(f\"\\nMean IVT correlation per node:\\n{np.round(mean_corr_ivt_per_node.T, 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nwgro86l2mn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mean residuals for flash flood days (with mean QE and correlation in titles)\n",
    "fig, axes = plt.subplots(\n",
    "    ydim,\n",
    "    xdim,\n",
    "    figsize=(6, 3.7),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    constrained_layout=True,\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Symmetric levels for residuals (centered on zero)\n",
    "levels_z500_resid = np.arange(-2.0, 2.05, 0.2)\n",
    "levels_ivt_resid = np.arange(-2.0, 2.05, 0.2)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        z500_resid_field = z500_residuals[i, j, :, :]\n",
    "        ivt_resid_field = ivt_residuals[i, j, :, :]\n",
    "\n",
    "        # Skip nodes with no flash flood days\n",
    "        if np.isnan(z500_resid_field).all():\n",
    "            ax.set_title(f\"({i},{j})  n=0\", fontsize=5)\n",
    "            ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "            ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # IVT residuals shaded\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_resid_field,\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_ivt_resid,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"both\",\n",
    "        )\n",
    "\n",
    "        # Z500 residuals contoured\n",
    "        ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            z500_resid_field,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_z500_resid,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        ax.add_feature(cfeature.COASTLINE, linewidth=0.6)\n",
    "        ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "        ax.set_title(\n",
    "            f\"({i},{j}) n={ff_counts[i, j]} r={mean_corr_per_node[i, j]:.2f}\",\n",
    "            fontsize=5,\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"Standardized Residual\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Mean Residuals (FFE days): Z500 (contoured) + IVT (shaded)\", fontsize=8, y=1.04\n",
    ")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/residuals_ffe.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "jr45wa2r9yl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Flood Events Ranked by Spatial Correlation (Worst Pattern Match First)\n",
      "=====================================================================================\n",
      "Rank  Date          Node      Corr    Z500_r  IVT_r   QE        \n",
      "-------------------------------------------------------------------------------------\n",
      "1     2012-06-22    (3,1)     -0.208   -0.148   -0.133   98.1\n",
      "2     2017-08-02    (3,1)     -0.147   0.152   -0.224   111.6\n",
      "3     2013-05-09    (2,1)     -0.079   -0.444   0.367   100.0\n",
      "4     2000-07-03    (3,2)     0.062   0.252   -0.069   91.9\n",
      "5     2004-07-02    (2,2)     0.086   0.027   0.056   92.6\n",
      "6     2019-07-31    (2,1)     0.099   -0.186   0.278   117.1\n",
      "7     2006-06-02    (2,2)     0.187   0.032   -0.268   86.3\n",
      "8     1996-09-08    (3,0)     0.197   0.136   0.300   100.4\n",
      "9     2005-07-06    (3,0)     0.216   0.301   0.447   116.3\n",
      "10    2018-06-28    (1,1)     0.220   0.176   0.132   78.1\n",
      "11    2002-06-26    (1,1)     0.220   0.018   0.267   92.4\n",
      "12    2021-08-22    (0,3)     0.225   0.221   0.081   138.0\n",
      "13    2000-09-03    (2,1)     0.228   0.207   0.330   82.4\n",
      "14    2022-07-18    (2,0)     0.240   0.148   0.284   116.4\n",
      "15    1996-10-19    (3,3)     0.241   0.295   0.023   132.7\n",
      "16    2011-08-20    (4,2)     0.241   -0.098   0.174   97.3\n",
      "17    1998-06-13    (3,0)     0.252   0.036   0.214   130.4\n",
      "18    2006-07-21    (1,1)     0.257   -0.138   0.216   85.4\n",
      "19    2020-07-10    (0,2)     0.264   -0.009   0.218   120.0\n",
      "20    1997-07-16    (1,2)     0.274   -0.077   0.407   83.5\n",
      "21    2013-05-08    (0,3)     0.276   0.181   -0.037   122.6\n",
      "22    2012-08-01    (3,2)     0.276   0.385   0.144   110.2\n",
      "23    2011-08-14    (4,2)     0.280   0.585   -0.011   87.6\n",
      "24    2018-08-11    (2,3)     0.307   0.322   0.130   100.0\n",
      "25    2019-07-18    (0,1)     0.314   0.732   0.173   112.7\n",
      "26    2006-06-29    (3,0)     0.315   0.644   0.363   116.6\n",
      "27    2010-08-23    (0,3)     0.330   0.397   0.089   148.2\n",
      "28    1998-08-17    (1,0)     0.337   0.669   0.029   99.8\n",
      "29    2002-09-02    (0,2)     0.355   0.141   0.612   100.6\n",
      "30    2001-06-17    (1,1)     0.358   0.281   0.152   105.6\n",
      "31    2004-09-28    (2,0)     0.359   0.251   0.495   117.4\n",
      "32    2015-07-30    (0,1)     0.369   0.542   0.381   127.8\n",
      "33    2004-09-18    (2,1)     0.378   0.174   0.654   109.1\n",
      "34    1999-08-26    (3,0)     0.380   0.402   0.253   103.2\n",
      "35    2005-10-14    (0,3)     0.399   0.517   0.161   100.6\n",
      "36    2024-08-06    (2,1)     0.418   0.682   0.239   173.6\n",
      "37    2007-07-18    (3,1)     0.424   0.378   0.309   82.3\n",
      "38    2006-07-12    (0,0)     0.426   0.509   0.328   124.4\n",
      "39    2004-09-08    (2,0)     0.428   0.352   0.579   129.4\n",
      "40    2006-08-25    (4,2)     0.434   0.236   0.233   98.2\n",
      "41    2018-07-17    (2,0)     0.435   0.358   0.568   78.8\n",
      "42    2013-09-02    (4,1)     0.439   0.702   0.384   83.3\n",
      "43    2018-08-04    (0,2)     0.455   0.099   0.469   122.6\n",
      "44    2012-07-18    (2,1)     0.469   0.495   0.489   90.1\n",
      "45    2021-06-08    (0,1)     0.470   0.403   0.241   148.4\n",
      "46    2015-05-31    (2,0)     0.471   0.457   0.628   109.9\n",
      "47    2010-10-12    (4,3)     0.478   0.593   -0.139   92.4\n",
      "48    2007-06-28    (1,0)     0.490   0.411   0.576   98.1\n",
      "49    2008-06-14    (0,0)     0.491   0.502   0.490   136.4\n",
      "50    1996-07-31    (3,0)     0.493   0.674   0.441   123.2\n",
      "51    2021-07-02    (3,2)     0.495   0.628   0.402   148.7\n",
      "52    2014-07-04    (3,0)     0.505   0.682   0.664   133.9\n",
      "53    2014-10-22    (1,3)     0.529   0.578   0.384   130.5\n",
      "54    2003-08-04    (3,0)     0.533   0.628   0.607   122.1\n",
      "55    2007-08-08    (1,1)     0.535   0.775   0.512   101.2\n",
      "56    2017-08-04    (4,0)     0.548   0.708   0.116   134.9\n",
      "57    2003-09-23    (4,0)     0.551   0.685   0.159   120.2\n",
      "58    2004-06-25    (4,0)     0.554   0.488   0.354   128.8\n",
      "59    2011-08-22    (4,1)     0.560   0.451   0.311   114.2\n",
      "60    2013-06-03    (2,0)     0.564   0.758   0.468   93.3\n",
      "61    2001-08-13    (2,0)     0.569   0.754   0.469   93.6\n",
      "62    2014-05-17    (4,0)     0.569   0.884   0.172   157.5\n",
      "63    2016-07-25    (0,1)     0.569   0.579   0.414   89.7\n",
      "64    2021-08-27    (0,1)     0.578   0.681   0.213   97.7\n",
      "65    2019-07-22    (2,0)     0.590   0.720   0.409   80.4\n",
      "66    2021-07-08    (3,0)     0.596   0.795   0.654   101.1\n",
      "67    2008-08-15    (4,3)     0.596   0.574   0.551   118.0\n",
      "68    2000-08-28    (2,3)     0.596   0.687   0.509   100.0\n",
      "69    2007-07-11    (3,0)     0.601   0.554   0.395   146.5\n",
      "70    2012-09-08    (4,0)     0.603   0.675   0.182   114.7\n",
      "71    2019-08-07    (4,0)     0.605   0.644   0.241   104.0\n",
      "72    2000-08-11    (2,3)     0.606   0.693   0.388   100.7\n",
      "73    1996-07-08    (4,1)     0.611   0.207   0.104   135.6\n",
      "74    2010-10-01    (4,0)     0.612   0.589   0.576   154.1\n",
      "75    2014-06-13    (4,0)     0.615   0.790   0.274   96.4\n",
      "76    2018-08-07    (0,0)     0.616   0.602   0.580   86.0\n",
      "77    2022-09-13    (4,0)     0.620   0.753   0.696   83.4\n",
      "78    2007-10-11    (4,3)     0.625   0.774   0.302   85.5\n",
      "79    2006-08-11    (4,3)     0.636   0.679   0.382   122.6\n",
      "80    2011-08-01    (1,3)     0.643   0.608   0.532   105.8\n",
      "81    2012-08-15    (4,0)     0.654   0.677   0.447   76.2\n",
      "82    2014-05-10    (1,0)     0.656   0.851   0.620   100.0\n",
      "83    2023-07-16    (3,0)     0.656   0.656   0.562   130.4\n",
      "84    2014-07-03    (3,0)     0.661   0.858   0.645   131.1\n",
      "85    2021-09-01    (4,1)     0.675   0.644   0.597   116.4\n",
      "86    2004-08-11    (4,0)     0.678   0.709   0.104   150.4\n",
      "87    2008-07-28    (4,3)     0.679   0.820   0.409   69.5\n",
      "88    2004-06-17    (1,0)     0.687   0.831   0.560   96.6\n",
      "89    2019-07-23    (4,1)     0.695   0.770   0.549   96.1\n",
      "90    2017-05-05    (4,0)     0.697   0.648   0.732   140.2\n",
      "91    2014-08-31    (0,0)     0.704   0.853   0.637   124.2\n",
      "92    2008-09-06    (4,0)     0.707   0.700   0.793   161.0\n",
      "93    2012-06-07    (4,3)     0.708   0.883   0.699   105.7\n",
      "94    2003-08-17    (3,3)     0.709   0.938   0.314   109.1\n",
      "95    2018-07-27    (3,0)     0.711   0.889   0.524   95.0\n",
      "96    2018-09-28    (2,0)     0.721   0.843   0.513   101.5\n",
      "97    2008-06-22    (4,0)     0.723   0.914   0.479   73.8\n",
      "98    2008-08-14    (4,3)     0.727   0.630   0.761   155.6\n",
      "99    1999-09-16    (4,0)     0.736   0.777   0.831   155.7\n",
      "100   2009-07-29    (4,0)     0.742   0.716   0.456   147.1\n",
      "101   1996-07-13    (4,0)     0.749   0.799   0.723   116.4\n",
      "102   2008-08-11    (4,3)     0.755   0.774   0.550   118.3\n",
      "103   2013-05-23    (3,0)     0.757   0.892   0.755   115.7\n",
      "104   1996-07-03    (4,3)     0.774   0.839   0.475   139.4\n",
      "105   2023-09-29    (0,3)     0.778   0.617   0.734   84.8\n",
      "106   2006-10-28    (4,0)     0.778   0.741   0.662   132.1\n",
      "107   2015-07-15    (4,2)     0.791   0.690   0.782   137.0\n",
      "108   2017-06-19    (3,0)     0.795   0.878   0.648   166.2\n",
      "109   2018-09-25    (0,0)     0.809   0.897   0.707   93.4\n",
      "110   2002-08-16    (0,0)     0.823   0.898   0.772   114.4\n",
      "111   2003-07-22    (4,0)     0.837   0.947   0.627   110.7\n",
      "112   2007-06-04    (4,0)     0.837   0.835   0.606   135.2\n",
      "113   2009-07-27    (4,0)     0.838   0.920   0.638   81.0\n",
      "114   2014-07-15    (4,0)     0.844   0.810   0.686   128.2\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "Total flash flood events: 114\n",
      "Mean correlation (all FFE): 0.504\n",
      "Mean Z500 correlation: 0.533\n",
      "Mean IVT correlation: 0.396\n",
      "Mean QE (all FFE): 112.57\n"
     ]
    }
   ],
   "source": [
    "# Rank flash flood events by spatial correlation (worst pattern match = lowest correlation)\n",
    "print(\"Flash Flood Events Ranked by Spatial Correlation (Worst Pattern Match First)\")\n",
    "print(\"=\" * 85)\n",
    "print(\n",
    "    f\"{'Rank':<6}{'Date':<14}{'Node':<10}{'Corr':<8}{'Z500_r':<8}{'IVT_r':<8}{'QE':<10}\"\n",
    ")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for rank, (_, row) in enumerate(event_metrics_df_by_corr.iterrows(), start=1):\n",
    "    date_str = row[\"date\"].strftime(\"%Y-%m-%d\")\n",
    "    node_str = f\"({int(row['node_i'])},{int(row['node_j'])})\"\n",
    "    print(\n",
    "        f\"{rank:<6}{date_str:<14}{node_str:<10}{row['corr']:.3f}   \"\n",
    "        f\"{row['corr_z500']:.3f}   {row['corr_ivt']:.3f}   {row['qe']:.1f}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 85)\n",
    "print(f\"\\nTotal flash flood events: {len(event_metrics_df)}\")\n",
    "print(f\"Mean correlation (all FFE): {event_metrics_df['corr'].mean():.3f}\")\n",
    "print(f\"Mean Z500 correlation: {event_metrics_df['corr_z500'].mean():.3f}\")\n",
    "print(f\"Mean IVT correlation: {event_metrics_df['corr_ivt'].mean():.3f}\")\n",
    "print(f\"Mean QE (all FFE): {event_metrics_df['qe'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c4b4fq1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top N highest-QE flash flood events to visually inspect for cutoff lows\n",
    "n_top = 15\n",
    "top_events = event_qe_df.head(n_top)\n",
    "\n",
    "cols = 5\n",
    "rows = int(np.ceil(n_top / cols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    rows,\n",
    "    cols,\n",
    "    figsize=(3 * cols, 2.5 * rows),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    layout=\"constrained\",\n",
    "    dpi=300,\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Levels for Z500 contours (in dam) - extended range for cutoff lows\n",
    "levels_Z = range(534, 606, 3)\n",
    "levels_ivt = np.arange(0, 701, 100)\n",
    "\n",
    "for k, (_, row) in enumerate(top_events.iterrows()):\n",
    "    ax = axes[k]\n",
    "\n",
    "    # Find the time index for this date\n",
    "    event_date = row[\"date\"]\n",
    "    t = np.where(pd.to_datetime(Z500_daily.time.values) == event_date)[0][0]\n",
    "\n",
    "    z500_data = Z500_daily.isel(time=t)\n",
    "    ivt_data = IVT_daily.isel(valid_time=t)\n",
    "\n",
    "    # IVT shaded\n",
    "    im = ax.contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        ivt_data,\n",
    "        cmap=\"BuPu\",\n",
    "        levels=levels_ivt,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        extend=\"max\",\n",
    "    )\n",
    "\n",
    "    # Z500 contoured\n",
    "    cn = ax.contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        z500_data / 98.1,\n",
    "        colors=\"black\",\n",
    "        linewidths=0.6,\n",
    "        levels=levels_Z,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.3)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{event_date.strftime('%Y-%m-%d')}\\nNode ({int(row['node_i'])},{int(row['node_j'])})  QE={row['qe']:.1f}\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "# Turn off any unused axes\n",
    "for k in range(n_top, len(axes)):\n",
    "    axes[k].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Top {n_top} Highest-QE Flash Flood Events\", fontsize=10, y=1.02)\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/top_qe_events.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "oottt45lz9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bottom N lowest-QE flash flood events (best fit) for comparison\n",
    "n_bottom = 15\n",
    "bottom_events = event_qe_df.tail(n_bottom).iloc[::-1]  # Reverse so lowest QE is first\n",
    "\n",
    "cols = 5\n",
    "rows = int(np.ceil(n_bottom / cols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    rows,\n",
    "    cols,\n",
    "    figsize=(3 * cols, 2.5 * rows),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    layout=\"constrained\",\n",
    "    dpi=300,\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Levels for Z500 contours (in dam) - extended range\n",
    "levels_Z = range(534, 606, 3)\n",
    "levels_ivt = np.arange(0, 701, 100)\n",
    "\n",
    "for k, (_, row) in enumerate(bottom_events.iterrows()):\n",
    "    ax = axes[k]\n",
    "\n",
    "    # Find the time index for this date\n",
    "    event_date = row[\"date\"]\n",
    "    t = np.where(pd.to_datetime(Z500_daily.time.values) == event_date)[0][0]\n",
    "\n",
    "    z500_data = Z500_daily.isel(time=t)\n",
    "    ivt_data = IVT_daily.isel(valid_time=t)\n",
    "\n",
    "    # IVT shaded\n",
    "    im = ax.contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        ivt_data,\n",
    "        cmap=\"BuPu\",\n",
    "        levels=levels_ivt,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        extend=\"max\",\n",
    "    )\n",
    "\n",
    "    # Z500 contoured\n",
    "    cn = ax.contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        z500_data / 98.1,\n",
    "        colors=\"black\",\n",
    "        linewidths=0.6,\n",
    "        levels=levels_Z,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.3)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{event_date.strftime('%Y-%m-%d')}\\nNode ({int(row['node_i'])},{int(row['node_j'])})  QE={row['qe']:.1f}\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "# Turn off any unused axes\n",
    "for k in range(n_bottom, len(axes)):\n",
    "    axes[k].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Bottom {n_bottom} Lowest-QE Flash Flood Events (Best Fit)\", fontsize=10, y=1.02\n",
    ")\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/bottom_qe_events.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tm2h2u0ap3",
   "metadata": {},
   "source": [
    "### Cutoff Low Detection\n",
    "\n",
    "Simple automated detection of cutoff lows in flash flood events. A cutoff low is identified when there is a single local minimum in the Z500 field that is surrounded by higher values (i.e., enclosed by a closed contour using 6-dam spacing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ofcn1m97dj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff low detection results:\n",
      "Total flash flood events: 114\n",
      "Events with cutoff lows: 5 (4.4%)\n",
      "\n",
      "Cutoff fraction by node:\n",
      "[[ 0.   0.   0.  13.3  0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [20.  50.   0.  50.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def has_cutoff_low(z500_field, contour_spacing=6):\n",
    "    \"\"\"\n",
    "    Detect if a Z500 field contains a cutoff low.\n",
    "\n",
    "    A cutoff low is identified as a local minimum where the contour\n",
    "    at (minimum + contour_spacing) forms a closed loop that does not\n",
    "    touch the domain boundary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z500_field : array-like\n",
    "        2D array of Z500 values in dam (decameters)\n",
    "    contour_spacing : float\n",
    "        Contour interval in dam. The closed contour is defined at\n",
    "        (local_minimum + contour_spacing).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if a cutoff low is detected, False otherwise\n",
    "    \"\"\"\n",
    "    z500 = np.asarray(z500_field)\n",
    "\n",
    "    # Find local minima using minimum filter\n",
    "    min_filtered = ndimage.minimum_filter(z500, size=5)\n",
    "    local_minima = z500 == min_filtered\n",
    "\n",
    "    # Get coordinates of local minima\n",
    "    min_coords = np.argwhere(local_minima)\n",
    "\n",
    "    for coord in min_coords:\n",
    "        i, j = coord\n",
    "        min_val = z500[i, j]\n",
    "\n",
    "        # Skip minima too close to the edge\n",
    "        if i < 2 or i >= z500.shape[0] - 2 or j < 2 or j >= z500.shape[1] - 2:\n",
    "            continue\n",
    "\n",
    "        # Define the closed contour level: minimum + contour_spacing\n",
    "        contour_level = min_val + contour_spacing\n",
    "\n",
    "        # Find the region enclosed by this contour (values < contour_level)\n",
    "        below_contour = z500 < contour_level\n",
    "\n",
    "        # Label connected regions\n",
    "        labeled, num_features = ndimage.label(below_contour)\n",
    "\n",
    "        if num_features == 0:\n",
    "            continue\n",
    "\n",
    "        # Get the label of the region containing this minimum\n",
    "        region_label = labeled[i, j]\n",
    "\n",
    "        if region_label == 0:\n",
    "            continue\n",
    "\n",
    "        # Check if this region touches any boundary\n",
    "        region_mask = labeled == region_label\n",
    "\n",
    "        touches_boundary = (\n",
    "            np.any(region_mask[0, :])  # top edge\n",
    "            or np.any(region_mask[-1, :])  # bottom edge\n",
    "            or np.any(region_mask[:, 0])  # left edge\n",
    "            or np.any(region_mask[:, -1])  # right edge\n",
    "        )\n",
    "\n",
    "        # If the region doesn't touch any boundary, it's a closed cutoff\n",
    "        if not touches_boundary:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Test on all flash flood events and track by node\n",
    "cutoff_by_node = {\n",
    "    (i, j): {\"total\": 0, \"cutoff\": 0} for i in range(xdim) for j in range(ydim)\n",
    "}\n",
    "cutoff_events = []  # Store (date, node_i, node_j, has_cutoff)\n",
    "\n",
    "for idx in event_indices:\n",
    "    # Get Z500 in dam\n",
    "    z500_dam = Z500_daily.isel(time=idx).values / 98.1\n",
    "\n",
    "    # Get BMU for this day\n",
    "    node_i, node_j = bmus[idx]\n",
    "\n",
    "    # Check for cutoff low\n",
    "    has_cutoff = has_cutoff_low(z500_dam, contour_spacing=6)\n",
    "\n",
    "    # Update counts\n",
    "    cutoff_by_node[(node_i, node_j)][\"total\"] += 1\n",
    "    if has_cutoff:\n",
    "        cutoff_by_node[(node_i, node_j)][\"cutoff\"] += 1\n",
    "\n",
    "    # Store event info\n",
    "    event_date = pd.to_datetime(Z500_daily.time.values[idx])\n",
    "    cutoff_events.append((event_date, node_i, node_j, has_cutoff))\n",
    "\n",
    "# Create arrays for visualization\n",
    "cutoff_fraction = np.full((xdim, ydim), np.nan)\n",
    "cutoff_count = np.zeros((xdim, ydim), dtype=int)\n",
    "total_ff_count = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        total = cutoff_by_node[(i, j)][\"total\"]\n",
    "        cutoff = cutoff_by_node[(i, j)][\"cutoff\"]\n",
    "        total_ff_count[i, j] = total\n",
    "        cutoff_count[i, j] = cutoff\n",
    "        if total > 0:\n",
    "            cutoff_fraction[i, j] = cutoff / total\n",
    "\n",
    "# Summary\n",
    "total_cutoffs = sum(1 for e in cutoff_events if e[3])\n",
    "print(f\"Cutoff low detection results:\")\n",
    "print(f\"Total flash flood events: {len(cutoff_events)}\")\n",
    "print(\n",
    "    f\"Events with cutoff lows: {total_cutoffs} ({100 * total_cutoffs / len(cutoff_events):.1f}%)\"\n",
    ")\n",
    "print(f\"\\nCutoff fraction by node:\")\n",
    "print(f\"{np.round(cutoff_fraction.T * 100, 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nynoadl4s8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cutoff low fraction by SOM node\n",
    "fig, ax = plt.subplots(figsize=(5, 4), dpi=150)\n",
    "\n",
    "# Plot cutoff fraction as heatmap\n",
    "im = ax.imshow(\n",
    "    cutoff_fraction.T * 100,\n",
    "    cmap=\"YlOrRd\",\n",
    "    origin=\"lower\",\n",
    "    vmin=0,\n",
    "    vmax=100,\n",
    ")\n",
    "\n",
    "# Add text annotations with fraction and counts\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        total = total_ff_count[i, j]\n",
    "        cutoff = cutoff_count[i, j]\n",
    "        frac = cutoff_fraction[i, j]\n",
    "        \n",
    "        if total > 0:\n",
    "            text = f\"{100 * frac:.0f}\\\\%\\n({cutoff}/{total})\"\n",
    "        else:\n",
    "            text = \"n=0\"\n",
    "        \n",
    "        # Choose text color based on background\n",
    "        text_color = \"white\" if frac > 0.5 else \"black\"\n",
    "        ax.text(i, j, text, ha=\"center\", va=\"center\", fontsize=7, color=text_color)\n",
    "\n",
    "ax.set_xticks(np.arange(xdim))\n",
    "ax.set_yticks(np.arange(ydim))\n",
    "ax.set_xlabel(\"X-index\", fontsize=8)\n",
    "ax.set_ylabel(\"Y-index\", fontsize=8)\n",
    "ax.set_title(\"Cutoff Low Frequency in Flash Flood Events by SOM Node\", fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Cutoff Low Frequency (\\\\%)\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/Z500-IVT-big-SOM/cutoff_low_frequency.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a95e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soms314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
