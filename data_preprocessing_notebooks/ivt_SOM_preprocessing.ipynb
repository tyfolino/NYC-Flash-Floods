{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6815d476",
   "metadata": {},
   "source": [
    "# Notebook for TCWV and VIWV SOM Preprocessing\n",
    "\n",
    "By: Ty Janoski\n",
    "\n",
    "Updated 1/2/2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2562b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a5884",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aada540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import cartopy.crs as ccrs\n",
    "import cmweather  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots  # noqa: F401\n",
    "import xarray as xr\n",
    "from dask.diagnostics.progress import ProgressBar  # noqa: F401\n",
    "\n",
    "plt.style.use([\"science\", \"nature\", \"grid\"])\n",
    "plt.rcParams[\"text.usetex\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421b468",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0bb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV file with flash flood events\n",
    "df = pd.read_csv(\"../data/storm_data_search_results.csv\")\n",
    "\n",
    "# Remove rows where EVENT_ID is not a digit\n",
    "df = df[df[\"EVENT_ID\"].astype(str).str.isdigit()]\n",
    "\n",
    "# Turn BEGIN_TIME and END_TIME into strings with leading zeros if necessary\n",
    "df[\"BEGIN_TIME\"] = df[\"BEGIN_TIME\"].fillna(0).astype(int).astype(str).str.zfill(4)\n",
    "df[\"END_TIME\"] = df[\"END_TIME\"].fillna(0).astype(int).astype(str).str.zfill(4)\n",
    "\n",
    "# Combine TIME and DATE into a single datetime string\n",
    "begin_str = df[\"BEGIN_DATE\"] + \" \" + df[\"BEGIN_TIME\"] # type: ignore\n",
    "end_str = df[\"END_DATE\"] + \" \" + df[\"END_TIME\"] # type: ignore\n",
    "\n",
    "# Convert the datetime strings to pandas datetime objects\n",
    "df[\"BEGIN_DATETIME\"] = pd.to_datetime(\n",
    "    begin_str, format=\"%m/%d/%Y %H%M\", errors=\"coerce\"\n",
    ")\n",
    "df[\"END_DATETIME\"] = pd.to_datetime(end_str, format=\"%m/%d/%Y %H%M\", errors=\"coerce\")\n",
    "\n",
    "# Take only the first row for each EPISODE_ID\n",
    "df_unique = df.drop_duplicates(subset=[\"EPISODE_ID\"], keep=\"first\").copy()\n",
    "\n",
    "# Create a list of datetimes rounded to the nearest preceding hour\n",
    "event_hours = df_unique[\"BEGIN_DATETIME\"].dt.floor(\"h\").tolist() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ec15f",
   "metadata": {},
   "source": [
    "## ERA5 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e354ae8",
   "metadata": {},
   "source": [
    "### Total-Column Water Vapor (TCWV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d768b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run the following code if you need to reprocess the ERA5 data\n",
    "# --- IGNORE ---\n",
    "\n",
    "# # Read in hourly, sliced total-column water vapor (tcwv) data\n",
    "# tcwv = xr.open_mfdataset(\n",
    "#     \"/mnt/drive2/ERA5/NC_files/hourly_sliced/era5_pw_*.nc\",\n",
    "#     combine=\"nested\",\n",
    "#     concat_dim=\"time\",\n",
    "#     decode_times=True,\n",
    "#     chunks={\"time\": 8760},\n",
    "# ).rename({\"var137\": \"tcwv\"})\n",
    "\n",
    "# # Sort by time\n",
    "# tcwv = tcwv.sortby(\"time\").tcwv.squeeze()\n",
    "\n",
    "# # Save out\n",
    "# with ProgressBar():\n",
    "#     tcwv.rename(\"tcwv\").load()\n",
    "\n",
    "# tcwv.to_netcdf(\n",
    "#     \"/mnt/drive2/ERA5/NC_files/combined/era5_tcwv_hourly_warm_season_US.nc\"\n",
    "# )\n",
    "\n",
    "tcwv = xr.load_dataarray(\n",
    "        \"/mnt/drive2/ERA5/NC_files/combined/era5_tcwv_hourly_warm_season_US.nc\",\n",
    "        decode_timedelta=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day of year to calculate mean and std dev\n",
    "mean_doy = tcwv.groupby(\"time.dayofyear\").mean(dim=\"time\")\n",
    "std_doy = tcwv.groupby(\"time.dayofyear\").std(dim=\"time\")\n",
    "\n",
    "std_doy_smooth = (\n",
    "    std_doy\n",
    "    .sortby(\"dayofyear\")\n",
    "    .rolling(dayofyear=14, center=True, min_periods=7)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Calculate standardized anomalies\n",
    "tcwv_anoms = tcwv.groupby(\"time.dayofyear\") - mean_doy\n",
    "tcwv_norm  = tcwv_anoms.groupby(\"time.dayofyear\") / std_doy_smooth\n",
    "\n",
    "# Weight by square root of cosine of latitude\n",
    "weights = np.sqrt(np.cos(np.deg2rad(tcwv_norm.lat)))\n",
    "tcwv_norm_weighted = tcwv_norm * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712122f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example plot of the standardized anomalies\n",
    "fig, axs = plt.subplots(\n",
    "    1, 3, figsize=(6.5, 3.5), subplot_kw={\"projection\": ccrs.PlateCarree()}, dpi=600\n",
    ")\n",
    "\n",
    "# Define plot configurations\n",
    "configs = [\n",
    "    {\n",
    "        \"data\": tcwv.isel(time=0),\n",
    "        \"lon\": tcwv.lon,\n",
    "        \"lat\": tcwv.lat,\n",
    "        \"title\": \"Z$_{500}$\",\n",
    "        \"cmap\": \"viridis\",\n",
    "        \"levels\": np.arange(0, 51, 5),\n",
    "    },\n",
    "    {\n",
    "        \"data\": tcwv_norm.isel(time=0),\n",
    "        \"lon\": tcwv_norm.lon,\n",
    "        \"lat\": tcwv_norm.lat,\n",
    "        \"title\": \"Z$_{500}$ Standardized Anomalies\",\n",
    "        \"cmap\": \"balance\",\n",
    "        \"levels\": np.arange(-3, 3.1, 0.5),\n",
    "    },\n",
    "    {\n",
    "        \"data\": tcwv_norm_weighted.isel(time=0),\n",
    "        \"lon\": tcwv_norm_weighted.lon,\n",
    "        \"lat\": tcwv_norm_weighted.lat,\n",
    "        \"title\": \"Z$_{500}$ Standardized Anomalies Weighted\",\n",
    "        \"cmap\": \"balance\",\n",
    "        \"levels\": np.arange(-3, 3.1, 0.5),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create plots and colorbars\n",
    "for i, config in enumerate(configs):\n",
    "    c = axs[i].contourf(\n",
    "        config[\"lon\"],\n",
    "        config[\"lat\"],\n",
    "        config[\"data\"],\n",
    "        cmap=config[\"cmap\"],\n",
    "        levels=config[\"levels\"],\n",
    "        extend=\"max\",\n",
    "    )\n",
    "    axs[i].set_title(config[\"title\"], fontsize=8)\n",
    "    axs[i].coastlines(resolution=\"50m\", linewidth=0.5)\n",
    "\n",
    "    cb = fig.colorbar(c, ax=axs[i], orientation=\"horizontal\", pad=0.03)\n",
    "    if i == 0:\n",
    "        cb.set_label(\"dam\", fontsize=6)\n",
    "    cb.set_ticks(c.levels)\n",
    "    cb.ax.tick_params(labelsize=6, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"figs/tcwv-SOM/tcwv_standardized_anomalies_example.png\",\n",
    "    dpi=600,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for event hours\n",
    "# The times are actual NYC local, so we need to convert them to UTC\n",
    "times_local = pd.to_datetime(event_hours).tz_localize(\"US/Eastern\")\n",
    "times_utc = times_local.tz_convert(\"UTC\").tz_convert(None)\n",
    "\n",
    "intersect = pd.Index(times_utc).intersection(tcwv_norm_weighted.indexes[\"time\"])\n",
    "tcwv_norm_weighted_ffe = tcwv_norm_weighted.sel(time=intersect)\n",
    "tcwv_norm_ffe = tcwv_norm.sel(time=intersect)\n",
    "tcwv_ffe = tcwv.sel(time=intersect)\n",
    "\n",
    "# Save out the extracted data\n",
    "tcwv_norm_weighted_ffe.to_netcdf(\n",
    "    \"/mnt/drive2/SOM_intermediate_files/era5_tcwv_norm_weighted_ffe.nc\"\n",
    ")\n",
    "tcwv_norm_ffe.to_netcdf(\"/mnt/drive2/SOM_intermediate_files/era5_tcwv_norm_ffe.nc\")\n",
    "tcwv_ffe.to_netcdf(\"/mnt/drive2/SOM_intermediate_files/era5_tcwv_ffe.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b16b40",
   "metadata": {},
   "source": [
    "### Vertically-Integrated Water Vapor Transport (VIWVN and VIWVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb885538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run the following code if you need to reprocess the ERA5 data\n",
    "# --- IGNORE ---\n",
    "\n",
    "# # Read in hourly, sliced vertically integrated water vapor transport\n",
    "# viwv = xr.open_mfdataset(\n",
    "#     \"/mnt/drive2/ERA5/NC_files/hourly_sliced/era5_viwv_*.nc\",\n",
    "#     combine=\"nested\",\n",
    "#     concat_dim=\"valid_time\",\n",
    "#     decode_times=True,\n",
    "#     chunks={\"valid_time\": 8760},\n",
    "# )\n",
    "\n",
    "# # Load into memory\n",
    "# with ProgressBar():\n",
    "#     viwv = viwv.load()\n",
    "\n",
    "# # Save out\n",
    "# viwv.to_netcdf(\n",
    "#     \"/mnt/drive2/ERA5/NC_files/combined/era5_viwv_hourly_warm_season_US.nc\"\n",
    "# )\n",
    "\n",
    "ds = xr.load_dataset(\n",
    "    \"/mnt/drive2/ERA5/NC_files/combined/era5_viwv_hourly_warm_season_US.nc\",\n",
    "    decode_timedelta=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1714f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add magnitude variable to dataset\n",
    "ds[\"ivt\"] = np.sqrt(ds[\"viwve\"] ** 2 + ds[\"viwvn\"] ** 2)\n",
    "\n",
    "# Group by day of year to calculate mean and std dev\n",
    "mean_doy = ds.groupby(\"valid_time.dayofyear\").mean(dim=\"valid_time\")\n",
    "std_doy = ds.groupby(\"valid_time.dayofyear\").std(dim=\"valid_time\")\n",
    "\n",
    "std_doy_smooth = (\n",
    "    std_doy.sortby(\"dayofyear\").rolling(dayofyear=14, center=True, min_periods=7).mean()\n",
    ")\n",
    "\n",
    "# Calculate standardized anomalies\n",
    "anoms = ds.groupby(\"valid_time.dayofyear\") - mean_doy\n",
    "norm = anoms.groupby(\"valid_time.dayofyear\") / std_doy_smooth\n",
    "\n",
    "# Weight by square root of cosine of latitude\n",
    "weights = np.sqrt(np.cos(np.deg2rad(norm.latitude)))\n",
    "norm_weighted = norm * weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b23cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    3, 3, figsize=(6.5, 6),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "components = [\n",
    "    (\"ivt\", \"IVT$_{mag}$\", np.arange(0, 701, 50), np.arange(-6, 6.1, 1.0), \"max\"),\n",
    "    (\"viwve\", \"IVT$_{x}$\", np.arange(-700, 701, 100), np.arange(-6, 6.1, 1.0), \"both\"),\n",
    "    (\"viwvn\", \"IVT$_{y}$\", np.arange(-700, 701, 100), np.arange(-6, 6.1, 1.0), \"both\"),\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    (ds, \"\"),\n",
    "    (norm, \" Standardized Anomalies\"),\n",
    "    (norm_weighted, \" Standardized Anomalies Weighted\"),\n",
    "]\n",
    "\n",
    "for row, (var, label, raw_levels, std_levels, extend) in enumerate(components):\n",
    "    for col, (dset, suffix) in enumerate(datasets):\n",
    "\n",
    "        ax = axs[row, col]\n",
    "        data = getattr(dset, var).isel(valid_time=0)\n",
    "\n",
    "        levels = raw_levels if col == 0 else std_levels\n",
    "        cmap = \"viridis\" if (row == 0 and col == 0) else \"balance\"\n",
    "\n",
    "        c = ax.contourf(\n",
    "            data.longitude,\n",
    "            data.latitude,\n",
    "            data,\n",
    "            cmap=cmap,\n",
    "            levels=levels,\n",
    "            extend=extend,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{label}{suffix}\", fontsize=8)\n",
    "        ax.coastlines(resolution=\"50m\", linewidth=0.5)\n",
    "\n",
    "        cb = fig.colorbar(c, ax=ax, orientation=\"horizontal\", pad=0.03)\n",
    "        if col == 0:\n",
    "            cb.set_label(\"kg m$^{-1}$ s$^{-1}$\", fontsize=6)\n",
    "\n",
    "        cb.set_ticks(levels)\n",
    "        cb.ax.tick_params(labelsize=6, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../figs/Z500-and-ivtxy-SOM/ivt_standardized_anomalies_example.png\",\n",
    "    dpi=600,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5856f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for event hours\n",
    "# The times are actual NYC local, so we need to convert them to UTC\n",
    "times_local = pd.to_datetime(event_hours).tz_localize(\"US/Eastern\")\n",
    "times_utc = times_local.tz_convert(\"UTC\").tz_convert(None)\n",
    "\n",
    "intersect = pd.Index(times_utc).intersection(norm_weighted.indexes[\"valid_time\"])\n",
    "norm_weighted_ffe = norm_weighted.sel(valid_time=intersect)\n",
    "norm_ffe = norm.sel(valid_time=intersect)\n",
    "ffe = ds.sel(valid_time=intersect)\n",
    "\n",
    "# Save out the extracted data\n",
    "norm_weighted_ffe.to_netcdf(\n",
    "    \"/mnt/drive2/SOM_intermediate_files/era5_ivt_norm_weighted_ffe.nc\"\n",
    ")\n",
    "norm_ffe.to_netcdf(\"/mnt/drive2/SOM_intermediate_files/era5_ivt_norm_ffe.nc\")\n",
    "ffe.to_netcdf(\"/mnt/drive2/SOM_intermediate_files/era5_ivt_ffe.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c275dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out daily averaged raw Z500 and unweighted standardized anomalies\n",
    "ds_daily = (\n",
    "    ds.resample(valid_time=\"1D\", label=\"left\", closed=\"left\")\n",
    "    .mean()\n",
    "    .dropna(dim=\"valid_time\", how=\"all\")\n",
    ")\n",
    "norm_daily = (\n",
    "    norm.resample(valid_time=\"1D\", label=\"left\", closed=\"left\")\n",
    "    .mean()\n",
    "    .dropna(dim=\"valid_time\", how=\"all\")\n",
    ")\n",
    "norm_weighted_daily = (\n",
    "    norm_weighted.resample(valid_time=\"1D\", label=\"left\", closed=\"left\")\n",
    "    .mean()\n",
    "    .dropna(dim=\"valid_time\", how=\"all\")\n",
    ")\n",
    "\n",
    "ds_daily.to_netcdf(\"/mnt/drive2/SOM_intermediate_files/era5_ivt_daily.nc\")\n",
    "norm_daily.to_netcdf(\"/mnt/drive2/SOM_intermediate_files/era5_ivt_norm_daily.nc\")\n",
    "norm_weighted_daily.to_netcdf(\n",
    "    \"/mnt/drive2/SOM_intermediate_files/era5_ivt_norm_weighted_daily.nc\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641cd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soms314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
