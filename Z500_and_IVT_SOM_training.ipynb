{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4692b8",
   "metadata": {},
   "source": [
    "# Flash Flood Only SOM Training (Z<sub>500</sub> and |IVT|)\n",
    "\n",
    "By: Ty Janoski\n",
    "\n",
    "Updated 1/24/2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efd764",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb4540",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7986ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmweather  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots  # noqa: F401\n",
    "import xarray as xr\n",
    "from minisom import MiniSom\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "plt.style.use([\"science\", \"nature\", \"grid\"])\n",
    "plt.rcParams[\"text.usetex\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bvyhchzpwzp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def get_node_indices(bmus, i, j):\n",
    "    \"\"\"Get sample indices belonging to node (i,j).\"\"\"\n",
    "    return np.where((bmus[:, 0] == i) & (bmus[:, 1] == j))[0]\n",
    "\n",
    "\n",
    "def compute_composites(data, bmus, xdim, ydim, time_dim=\"valid_time\"):\n",
    "    \"\"\"Compute composite mean for each SOM node.\"\"\"\n",
    "    sample_shape = data.isel({time_dim: 0}).shape\n",
    "    composites = np.full((xdim, ydim) + sample_shape, np.nan)\n",
    "    counts = np.zeros((xdim, ydim), dtype=int)\n",
    "\n",
    "    for i in range(xdim):\n",
    "        for j in range(ydim):\n",
    "            idx = get_node_indices(bmus, i, j)\n",
    "            counts[i, j] = len(idx)\n",
    "            if len(idx) > 0:\n",
    "                composites[i, j] = data.isel({time_dim: idx}).mean(time_dim).values\n",
    "    return composites, counts\n",
    "\n",
    "\n",
    "def create_som_figure(xdim, ydim, figsize=(6, 3.7), dpi=600):\n",
    "    \"\"\"Create a standard figure for SOM node plots.\"\"\"\n",
    "    fig, axes = plt.subplots(\n",
    "        ydim,\n",
    "        xdim,\n",
    "        figsize=figsize,\n",
    "        subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "        constrained_layout=True,\n",
    "        dpi=dpi,\n",
    "    )\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def add_map_features(ax):\n",
    "    \"\"\"Add standard map features to an axis.\"\"\"\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.4)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def plot_node_events(\n",
    "    data,\n",
    "    bmus,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    lon,\n",
    "    lat,\n",
    "    levels,\n",
    "    cmap,\n",
    "    save_pattern,\n",
    "    scale=1.0,\n",
    "    cbar_label=None,\n",
    "    time_dim=\"valid_time\",\n",
    "    contour=False,\n",
    "    z500_data=None,\n",
    "    z500_levels=None,\n",
    "    z500_scale=1.0,\n",
    "    z500_time_dim=\"time\",\n",
    "):\n",
    "    \"\"\"Plot individual events for each SOM node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z500_data : xarray.DataArray, optional\n",
    "        Z500 data to overlay as contours (same time dimension as data)\n",
    "    z500_levels : array-like, optional\n",
    "        Contour levels for Z500 (required if z500_data is provided)\n",
    "    z500_scale : float, optional\n",
    "        Scale factor for Z500 values (e.g., 1/98.1 to convert to dam)\n",
    "    z500_time_dim : str, optional\n",
    "        Name of time dimension in z500_data (default: \"time\")\n",
    "    \"\"\"\n",
    "    cols = 5\n",
    "    proj = ccrs.PlateCarree()\n",
    "\n",
    "    for i in range(xdim):\n",
    "        for j in range(ydim):\n",
    "            idx = get_node_indices(bmus, i, j)\n",
    "            n = len(idx)\n",
    "            rows = int(np.ceil(n / cols))\n",
    "\n",
    "            fig, axes = plt.subplots(\n",
    "                rows,\n",
    "                cols,\n",
    "                figsize=(3 * cols, 2.5 * rows),\n",
    "                subplot_kw={\"projection\": proj},\n",
    "                layout=\"constrained\",\n",
    "                dpi=300,\n",
    "            )\n",
    "\n",
    "            for k, ax in enumerate(axes.flat):\n",
    "                if k < n:\n",
    "                    field = data.isel({time_dim: idx[k]})\n",
    "                    time_val = field[time_dim].values\n",
    "\n",
    "                    if contour:\n",
    "                        im = ax.contour(\n",
    "                            lon,\n",
    "                            lat,\n",
    "                            field.values * scale,\n",
    "                            levels=levels,\n",
    "                            colors=\"black\",\n",
    "                            transform=proj,\n",
    "                            linewidths=0.6,\n",
    "                        )\n",
    "                        ax.clabel(im, im.levels, fontsize=5)\n",
    "                    else:\n",
    "                        im = ax.contourf(\n",
    "                            lon,\n",
    "                            lat,\n",
    "                            field.values * scale,\n",
    "                            levels=levels,\n",
    "                            cmap=cmap,\n",
    "                            transform=proj,\n",
    "                            extend=\"max\",\n",
    "                        )\n",
    "\n",
    "                        # Overlay Z500 contours if provided\n",
    "                        if z500_data is not None and z500_levels is not None:\n",
    "                            z500_field = z500_data.isel({z500_time_dim: idx[k]})\n",
    "                            cn = ax.contour(\n",
    "                                lon,\n",
    "                                lat,\n",
    "                                z500_field.values * z500_scale,\n",
    "                                levels=z500_levels,\n",
    "                                colors=\"black\",\n",
    "                                linewidths=0.5,\n",
    "                                transform=proj,\n",
    "                            )\n",
    "                            ax.clabel(cn, inline=True, fontsize=5, fmt=\"%.0f\")\n",
    "\n",
    "                    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "                    ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "                    ax.add_feature(cfeature.STATES, linewidth=0.2)\n",
    "                    ax.set_title(str(pd.to_datetime(time_val))[:16])\n",
    "                else:\n",
    "                    ax.axis(\"off\")\n",
    "\n",
    "            if cbar_label and not contour:\n",
    "                cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "                cbar.set_label(cbar_label, fontsize=6)\n",
    "\n",
    "            fig.suptitle(f\"Node ({i},{j})  N={n}\", fontsize=8, y=1.02)\n",
    "            plt.savefig(save_pattern.format(i=i, j=j))\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a515db",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e7b46a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Z500 and IVT at flash-flood event times\n",
    "path = \"/mnt/drive2/SOM_intermediate_files/\"\n",
    "ivt_norm_weighted_ffe = xr.load_dataset(f\"{path}era5_ivt_norm_weighted_ffe.nc\")[\"ivt\"]\n",
    "ivt_norm_ffe = xr.load_dataset(f\"{path}era5_ivt_norm_ffe.nc\")[\"ivt\"]\n",
    "ivt_ffe = xr.load_dataset(f\"{path}era5_ivt_ffe.nc\")[\"ivt\"]\n",
    "\n",
    "z500_norm_weighted_ffe = xr.load_dataarray(f\"{path}era5_Z500_norm_weighted_ffe.nc\")\n",
    "z500_norm_ffe = xr.load_dataarray(f\"{path}era5_Z500_norm_ffe.nc\")\n",
    "z500_ffe = xr.load_dataarray(f\"{path}era5_Z500_ffe.nc\")\n",
    "\n",
    "# Total precipitation and mean sea level pressure at flash-flood event times\n",
    "tp_ffe = xr.load_dataarray(f\"{path}era5_tp_ffe.nc\")\n",
    "mslp_ffe = xr.load_dataarray(f\"{path}era5_mslp_ffe.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5206a1",
   "metadata": {},
   "source": [
    "### Reshape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cd7b2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data for SOM training\n",
    "z500_flat = z500_norm_weighted_ffe.stack(\n",
    "    features=[\"lat\", \"lon\"]\n",
    ").values  # shape: (time, feature)\n",
    "ivt_flat = ivt_norm_weighted_ffe.stack(\n",
    "    features=[\"latitude\", \"longitude\"]\n",
    ").values  # shape: (time, features)\n",
    "\n",
    "# Concatenate the data\n",
    "X = np.concatenate((z500_flat, ivt_flat), axis=1)  # shape: (time, feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44096767",
   "metadata": {},
   "source": [
    "## SOM Training\n",
    "\n",
    "We are going to train our SOM with random initialization and online training. We will also use two phases: a \"coarse\" phase with a larger sigma and learning rate, then a \"fine\" phase with a smaller learning rate and sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1c099",
   "metadata": {},
   "source": [
    "### Set SOM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c57aa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SOM shape\n",
    "xdim, ydim = 2, 2\n",
    "\n",
    "# Set number of iterations for each phase\n",
    "# Using \"literature style\": short rough phase, longer fine phase with sigma=1\n",
    "n1, n2 = 1000, 1000\n",
    "\n",
    "# Set starting sigmas\n",
    "# Phase 1: large sigma for global ordering; Phase 2: sigma=1 for localized refinement\n",
    "sig1, sig2 = np.sqrt(xdim**2 + ydim**2), 1.0\n",
    "\n",
    "# Set starting learning rates\n",
    "lr1, lr2 = 0.5, 0.1\n",
    "\n",
    "# Random seed for reproducibility\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd0c59",
   "metadata": {},
   "source": [
    "### Train SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83fe49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 1000 / 1000 ] 100% - 0:00:00 left \n",
      " quantization error: 136.4534791604323\n",
      " [ 1000 / 1000 ] 100% - 0:00:00 left \n",
      " quantization error: 134.61211112981474\n"
     ]
    }
   ],
   "source": [
    "# Create SOM instance\n",
    "som = MiniSom(\n",
    "    xdim,\n",
    "    ydim,\n",
    "    input_len=X.shape[1],\n",
    "    sigma=sig1,\n",
    "    learning_rate=lr1,\n",
    "    decay_function=\"linear_decay_to_zero\",\n",
    "    sigma_decay_function=\"linear_decay_to_one\",\n",
    "    neighborhood_function=\"gaussian\",\n",
    "    random_seed=random_seed,\n",
    ")\n",
    "\n",
    "# Initialize random weights\n",
    "som.random_weights_init(X)\n",
    "\n",
    "# Random training\n",
    "som.train_random(X, n1, verbose=True)\n",
    "\n",
    "# Phase 2\n",
    "som._sigma = sig2 # type: ignore\n",
    "som._learning_rate = lr2\n",
    "som.train_random(X, n2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f85390",
   "metadata": {},
   "source": [
    "### Grab important fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5e5a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total node number\n",
    "n_nodes = xdim * ydim\n",
    "\n",
    "# Get flattened weights\n",
    "weights = som.get_weights().reshape(xdim * ydim, -1)\n",
    "\n",
    "# u-matrix\n",
    "u_matrix = som.distance_map().T\n",
    "\n",
    "# bmus & hit_map\n",
    "bmus = np.array([som.winner(x) for x in X])\n",
    "\n",
    "hit_map = np.zeros((xdim, ydim))\n",
    "for i, j in bmus:\n",
    "    hit_map[i, j] += 1\n",
    "hit_map = hit_map.T\n",
    "\n",
    "# Sammon Coordinates\n",
    "D = pairwise_distances(weights)\n",
    "coords = MDS(\n",
    "    n_components=2,\n",
    "    metric=\"precomputed\",\n",
    "    random_state=42,\n",
    "    n_init=4,\n",
    "    init=\"random\",  # pyright: ignore[reportCallIssue]\n",
    ").fit_transform(D)\n",
    "\n",
    "# Get lats/lons\n",
    "lat = ivt_norm_ffe.latitude\n",
    "lon = ivt_norm_ffe.longitude\n",
    "\n",
    "# Dimensions of the spatial field\n",
    "n_lat = lat.size\n",
    "n_lon = lon.size\n",
    "n_features = n_lat * n_lon\n",
    "\n",
    "# Split weights into Z500 and IVT components\n",
    "z500_weights = weights[:, :n_features]\n",
    "ivt_weights = weights[:, n_features:]\n",
    "\n",
    "# Reshape weights back to spatial dimensions\n",
    "z500_nodes = z500_weights.reshape(xdim, ydim, n_lat, n_lon)\n",
    "ivt_nodes = ivt_weights.reshape(xdim, ydim, n_lat, n_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mc662xekg6b",
   "metadata": {},
   "source": [
    "### Rotate SOM (Optional)\n",
    "\n",
    "Rotating a trained SOM does **not** violate SOM principles—it preserves the learned topology (neighbors remain neighbors). This is purely a relabeling of node coordinates for visualization purposes.\n",
    "\n",
    "The rotation below is **counterclockwise by 90°**. For a 2×2 grid, this maps:\n",
    "- `(0,0) → (0,1)`\n",
    "- `(0,1) → (1,1)`\n",
    "- `(1,0) → (0,0)`\n",
    "- `(1,1) → (1,0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2peg6mdg1jz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM rotated 90° clockwise.\n",
      "New BMU mapping:\n",
      "  Old (0,0) → New (0,1)\n",
      "  Old (0,1) → New (1,1)\n",
      "  Old (1,0) → New (0,0)\n",
      "  Old (1,1) → New (1,0)\n"
     ]
    }
   ],
   "source": [
    "# Set to True to apply clockwise rotation, False to skip\n",
    "ROTATE_SOM = True\n",
    "\n",
    "if ROTATE_SOM:\n",
    "    # Rotate node weight patterns clockwise (k=-1 means 90° CW)\n",
    "    # This must match the BMU transformation below\n",
    "    # np.rot90 rotates on axes (0, 1), which are our (i, j) node indices\n",
    "    z500_nodes = np.rot90(z500_nodes, k=-1, axes=(0, 1))\n",
    "    ivt_nodes = np.rot90(ivt_nodes, k=-1, axes=(0, 1))\n",
    "\n",
    "    # Transform BMU coordinates: (i, j) → (j, xdim - 1 - i)\n",
    "    # This transformation is consistent with 90° CW rotation of weights\n",
    "    bmus_new = np.column_stack([bmus[:, 1], xdim - 1 - bmus[:, 0]])\n",
    "    bmus = bmus_new\n",
    "\n",
    "    # Rotate u-matrix and hit_map for consistency\n",
    "    # Note: these were transposed earlier, so we rotate on the transposed arrays\n",
    "    u_matrix = np.rot90(u_matrix, k=-1)\n",
    "    hit_map = np.rot90(hit_map, k=-1)\n",
    "\n",
    "    # Recompute flattened weights in new order for Sammon map\n",
    "    weights_rotated = np.zeros_like(weights)\n",
    "    for i in range(xdim):\n",
    "        for j in range(ydim):\n",
    "            old_idx = i * ydim + j\n",
    "            new_idx = i * ydim + j\n",
    "            # Rebuild from rotated node arrays\n",
    "            weights_rotated[new_idx, :n_features] = z500_nodes[i, j].flatten()\n",
    "            weights_rotated[new_idx, n_features:] = ivt_nodes[i, j].flatten()\n",
    "    weights = weights_rotated\n",
    "\n",
    "    # Recompute Sammon/MDS coordinates with rotated weights\n",
    "    D = pairwise_distances(weights)\n",
    "    coords = MDS(\n",
    "        n_components=2,\n",
    "        metric=\"precomputed\",\n",
    "        random_state=42,\n",
    "        n_init=4,\n",
    "        init=\"random\",  # pyright: ignore[reportCallIssue]\n",
    "    ).fit_transform(D)\n",
    "\n",
    "    print(\"SOM rotated 90° clockwise.\")\n",
    "    print(\"New BMU mapping:\")\n",
    "    print(\"  Old (0,0) → New (0,1)\")\n",
    "    print(\"  Old (0,1) → New (1,1)\")\n",
    "    print(\"  Old (1,0) → New (0,0)\")\n",
    "    print(\"  Old (1,1) → New (1,0)\")\n",
    "else:\n",
    "    print(\"Rotation skipped (ROTATE_SOM = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8y54slrtuov",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 117 BMU assignments to data/som_2x2_bmus.csv\n"
     ]
    }
   ],
   "source": [
    "# Save BMUs with timestamps for cross-SOM analysis\n",
    "bmu_df = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": pd.to_datetime(ivt_ffe.valid_time.values),\n",
    "        \"node_i\": bmus[:, 0],\n",
    "        \"node_j\": bmus[:, 1],\n",
    "    }\n",
    ")\n",
    "bmu_df.to_csv(\"data/som_2x2_bmus.csv\", index=False)\n",
    "print(f\"Saved {len(bmu_df)} BMU assignments to data/som_2x2_bmus.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666d0b8",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6a70f",
   "metadata": {},
   "source": [
    "### U-matrix and Sammon Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b8563653",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, layout=\"constrained\", figsize=(6, 3), dpi=600)\n",
    "\n",
    "# u-matrix\n",
    "im0 = axes[0].imshow(u_matrix, cmap=\"viridis\", origin=\"lower\")\n",
    "axes[0].set_title(\"U-Matrix (Mean Inter-Node Distance)\", fontsize=7)\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04, shrink=0.7)\n",
    "\n",
    "# hit map\n",
    "im1 = axes[1].imshow(hit_map, cmap=\"plasma\", origin=\"lower\")\n",
    "axes[1].set_title(\"Hit Map (Samples per Node)\", fontsize=7)\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04, shrink=0.7)\n",
    "\n",
    "# axis styling\n",
    "for ax in axes:\n",
    "    ax.set_xticks(np.arange(xdim))\n",
    "    ax.set_yticks(np.arange(ydim))\n",
    "    ax.set_xlabel(\"X-index\", fontsize=6)\n",
    "    ax.set_ylabel(\"Y-index\", fontsize=6)\n",
    "\n",
    "plt.savefig(\"figs/Z500-and-ivt-SOM/Z500_ivt_som_u_matrix_hit_map.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7c3a9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten u-matrix & hit map\n",
    "U_flat = u_matrix.T.reshape(-1)  # back to (n_nodes,)\n",
    "hits_flat = hit_map.T.reshape(-1)  # back to (n_nodes,)\n",
    "\n",
    "# scale hits\n",
    "hits_scaled = 30 + 250 * (hits_flat / hits_flat.max())\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Scatter: U controls color, hits control bubble size\n",
    "sc = plt.scatter(\n",
    "    coords[:, 0],\n",
    "    coords[:, 1],\n",
    "    c=U_flat,\n",
    "    s=hits_scaled,\n",
    "    cmap=\"balance\",\n",
    "    edgecolor=\"k\",\n",
    "    linewidth=0.5,\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Draw lattice connections (right & down neighbors only)\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node = i * ydim + j\n",
    "\n",
    "        # right neighbor\n",
    "        if j + 1 < ydim:\n",
    "            nbr = i * ydim + (j + 1)\n",
    "            plt.plot(\n",
    "                [coords[node, 0], coords[nbr, 0]],\n",
    "                [coords[node, 1], coords[nbr, 1]],\n",
    "                \"k-\",\n",
    "                lw=0.6,\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "        # down neighbor\n",
    "        if i + 1 < xdim:\n",
    "            nbr = (i + 1) * ydim + j\n",
    "            plt.plot(\n",
    "                [coords[node, 0], coords[nbr, 0]],\n",
    "                [coords[node, 1], coords[nbr, 1]],\n",
    "                \"k-\",\n",
    "                lw=0.6,\n",
    "                alpha=0.4,\n",
    "            )\n",
    "\n",
    "# Node labels (i,j)\n",
    "for idx, (x, y) in enumerate(coords):\n",
    "    ix, iy = divmod(idx, ydim)\n",
    "    plt.text(x, y, f\"({ix},{iy})\", fontsize=8, ha=\"center\", va=\"center\", zorder=5)\n",
    "\n",
    "plt.title(\"Sammon / MDS Distortion Grid\\nU-Matrix (Color) \\\\& Node Frequency (Size)\")\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar(sc, label=\"U-Matrix (Avg. Neighbor Distance)\")\n",
    "plt.savefig(\"figs/Z500-and-ivt-SOM/Z500-ivt_som_sammon_mds.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3446c",
   "metadata": {},
   "source": [
    "### Node Weights Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f2885ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shading levels for standardized anomalies\n",
    "levels_ivt = np.arange(-1.8, 1.81, 0.2)\n",
    "levels_Z = np.arange(-1.4, 1.41, 0.2)\n",
    "\n",
    "fig, axes = create_som_figure(xdim, ydim)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # IVT shaded\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_nodes[i, j],\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_ivt,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        # Z500 contours\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            z500_nodes[i, j],\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "        ax.clabel(cn, inline=True, fontsize=5, fmt=\"%.1f\")\n",
    "\n",
    "        add_map_features(ax)\n",
    "        ax.set_title(f\"Node ({i},{j})\", fontsize=6)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"Standardized IVT Anomaly (Shaded)\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Flash Flood Only SOM: Node Weight Patterns\\nZ500 (contoured) + IVT (shaded)\",\n",
    "    fontsize=8,\n",
    ")\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/combined_node_weights_ivt_shaded.png\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20f67b",
   "metadata": {},
   "source": [
    "### Anomaly Composite Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c9de15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standardized anomaly composites\n",
    "z500_patterns, counts = compute_composites(\n",
    "    z500_norm_ffe, bmus, xdim, ydim, time_dim=\"time\"\n",
    ")\n",
    "ivt_patterns, _ = compute_composites(\n",
    "    ivt_norm_ffe, bmus, xdim, ydim, time_dim=\"valid_time\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cdc8ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levels for standardized anomaly composites\n",
    "levels_ivt_anom = np.arange(-2.5, 2.6, 0.5)\n",
    "levels_Z_anom = np.arange(-2.0, 2.1, 0.25)\n",
    "\n",
    "fig, axes = create_som_figure(xdim, ydim)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # IVT shaded\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_patterns[i, j],\n",
    "            cmap=\"balance\",\n",
    "            levels=levels_ivt_anom,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"both\",\n",
    "        )\n",
    "\n",
    "        # Z500 contours\n",
    "        ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            z500_patterns[i, j],\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z_anom,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "        add_map_features(ax)\n",
    "        ax.set_title(f\"({i},{j})  N={counts[i, j]}\", fontsize=6)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"Standardized Anomaly\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Flash Flood Only SOM Composite Anomalies: Z500 (contoured) + IVT (shaded)\",\n",
    "    fontsize=8,\n",
    "    y=1.04,\n",
    ")\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/Z500_and_ivt_SOM_composite_anomalies_ivt_shaded.png\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owuw5xwd8zf",
   "metadata": {},
   "source": [
    "### Node Representativeness\n",
    "\n",
    "To assess how well each SOM node represents its assigned events, we compute **spatial pattern correlations** between:\n",
    "1. The node's learned weight vector (prototype)\n",
    "2. The composite mean of all events assigned to that node\n",
    "\n",
    "For this multivariate SOM, we compute correlations separately for Z500 and IVT, as well as a combined correlation using the concatenated feature vectors. High correlations (r > 0.8) indicate that the prototype faithfully captures the typical pattern of its assigned events.\n",
    "\n",
    "**Note:** The weights were trained on latitude-weighted, standardized anomalies, while the composites here use unweighted standardized anomalies. Some differences are expected due to the weighting scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fas10kaf6h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "NODE REPRESENTATIVENESS: Pattern Correlations (Weights vs. Composites)\n",
      "===========================================================================\n",
      "\n",
      "Node        N    r(Z500)     r(IVT)  r(Combined) Interpretation      \n",
      "---------------------------------------------------------------------------\n",
      "(0,0)       25      0.357      0.825        0.565 Poor                \n",
      "(0,1)       33      0.684      0.695        0.633 Poor                \n",
      "(1,0)       35      0.960      0.974        0.963 Excellent           \n",
      "(1,1)       24      0.748      0.700        0.737 Moderate            \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Summary: Mean r(Combined) = 0.725, Min = 0.565, Max = 0.963\n",
      "→ Some nodes show poor representativeness; consider SOM size or training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Compute pattern correlations between node weights and composites\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute composites of the weighted data for a fair comparison\n",
    "# (weights were trained on latitude-weighted standardized anomalies)\n",
    "z500_patterns_weighted, node_counts = compute_composites(\n",
    "    z500_norm_weighted_ffe, bmus, xdim, ydim, time_dim=\"time\"\n",
    ")\n",
    "ivt_patterns_weighted, _ = compute_composites(\n",
    "    ivt_norm_weighted_ffe, bmus, xdim, ydim, time_dim=\"valid_time\"\n",
    ")\n",
    "\n",
    "# Store results\n",
    "representativeness = []\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"NODE REPRESENTATIVENESS: Pattern Correlations (Weights vs. Composites)\")\n",
    "print(\"=\" * 75)\n",
    "print(\n",
    "    f\"\\n{'Node':<8} {'N':>4} {'r(Z500)':>10} {'r(IVT)':>10} {'r(Combined)':>12} {'Interpretation':<20}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        n_events = int(node_counts[i, j])\n",
    "\n",
    "        # Flatten the spatial fields for correlation\n",
    "        z500_weight = z500_nodes[i, j].flatten()\n",
    "        ivt_weight = ivt_nodes[i, j].flatten()\n",
    "\n",
    "        z500_composite = z500_patterns_weighted[i, j].flatten()\n",
    "        ivt_composite = ivt_patterns_weighted[i, j].flatten()\n",
    "\n",
    "        # Compute correlations (handle potential NaNs)\n",
    "        if n_events > 0 and not np.any(np.isnan(z500_composite)):\n",
    "            r_z500, p_z500 = pearsonr(z500_weight, z500_composite)\n",
    "            r_ivt, p_ivt = pearsonr(ivt_weight, ivt_composite)\n",
    "\n",
    "            # Combined correlation using concatenated vectors\n",
    "            combined_weight = np.concatenate([z500_weight, ivt_weight])\n",
    "            combined_composite = np.concatenate([z500_composite, ivt_composite])\n",
    "            r_combined, p_combined = pearsonr(combined_weight, combined_composite)\n",
    "        else:\n",
    "            r_z500 = r_ivt = r_combined = np.nan\n",
    "            p_z500 = p_ivt = p_combined = np.nan\n",
    "\n",
    "        # Interpret the correlation\n",
    "        if r_combined >= 0.9:\n",
    "            interp = \"Excellent\"\n",
    "        elif r_combined >= 0.8:\n",
    "            interp = \"Good\"\n",
    "        elif r_combined >= 0.7:\n",
    "            interp = \"Moderate\"\n",
    "        else:\n",
    "            interp = \"Poor\"\n",
    "\n",
    "        representativeness.append(\n",
    "            {\n",
    "                \"node\": f\"({i},{j})\",\n",
    "                \"n\": n_events,\n",
    "                \"r_z500\": r_z500,\n",
    "                \"r_ivt\": r_ivt,\n",
    "                \"r_combined\": r_combined,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"({i},{j}){'':<4} {n_events:>4} {r_z500:>10.3f} {r_ivt:>10.3f} {r_combined:>12.3f} {interp:<20}\"\n",
    "        )\n",
    "\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Summary statistics\n",
    "r_combined_values = [\n",
    "    r[\"r_combined\"] for r in representativeness if not np.isnan(r[\"r_combined\"])\n",
    "]\n",
    "print(\n",
    "    f\"\\nSummary: Mean r(Combined) = {np.mean(r_combined_values):.3f}, \"\n",
    "    f\"Min = {np.min(r_combined_values):.3f}, Max = {np.max(r_combined_values):.3f}\"\n",
    ")\n",
    "\n",
    "if np.min(r_combined_values) >= 0.8:\n",
    "    print(\"→ All nodes show good-to-excellent representativeness.\")\n",
    "elif np.min(r_combined_values) >= 0.7:\n",
    "    print(\n",
    "        \"→ Most nodes show adequate representativeness; some may benefit from more events.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"→ Some nodes show poor representativeness; consider SOM size or training parameters.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1vafrlb1j91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representativeness figure to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_representativeness.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize representativeness\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3.5), dpi=600, constrained_layout=True)\n",
    "\n",
    "# Left panel: Heatmap of combined correlations on SOM grid\n",
    "ax = axes[0]\n",
    "r_grid = np.zeros((xdim, ydim))\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        idx = i * ydim + j\n",
    "        r_grid[i, j] = representativeness[idx][\"r_combined\"]\n",
    "\n",
    "# Display with origin=\"lower\" to match SOM convention\n",
    "im = ax.imshow(r_grid.T, cmap=\"RdYlGn\", vmin=0.5, vmax=1.0, origin=\"lower\")\n",
    "ax.set_xticks(np.arange(xdim))\n",
    "ax.set_yticks(np.arange(ydim))\n",
    "ax.set_xlabel(\"X-index\", fontsize=7)\n",
    "ax.set_ylabel(\"Y-index\", fontsize=7)\n",
    "ax.set_title(\"Combined Pattern Correlation\\n(Weights vs. Composites)\", fontsize=8)\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        r_val = r_grid[i, j]\n",
    "        color = \"white\" if r_val < 0.75 else \"black\"\n",
    "        ax.text(i, j, f\"{r_val:.2f}\", ha=\"center\", va=\"center\", fontsize=8, color=color)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Pearson r\", fontsize=7)\n",
    "\n",
    "# Right panel: Grouped bar chart showing Z500, IVT, and Combined correlations\n",
    "ax = axes[1]\n",
    "node_labels = [r[\"node\"] for r in representativeness]\n",
    "x = np.arange(len(node_labels))\n",
    "width = 0.25\n",
    "\n",
    "r_z500 = [r[\"r_z500\"] for r in representativeness]\n",
    "r_ivt = [r[\"r_ivt\"] for r in representativeness]\n",
    "r_comb = [r[\"r_combined\"] for r in representativeness]\n",
    "\n",
    "bars1 = ax.bar(x - width, r_z500, width, label=\"Z500\", color=\"steelblue\", alpha=0.9)\n",
    "bars2 = ax.bar(x, r_ivt, width, label=\"IVT\", color=\"coral\", alpha=0.9)\n",
    "bars3 = ax.bar(x + width, r_comb, width, label=\"Combined\", color=\"seagreen\", alpha=0.9)\n",
    "\n",
    "ax.axhline(0.8, color=\"gray\", linestyle=\"--\", linewidth=0.8, label=\"r = 0.8 threshold\")\n",
    "ax.set_xlabel(\"SOM Node\", fontsize=7)\n",
    "ax.set_ylabel(\"Pattern Correlation (r)\", fontsize=7)\n",
    "ax.set_title(\"Representativeness by Variable\", fontsize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(node_labels, fontsize=6)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.legend(fontsize=6, loc=\"lower right\")\n",
    "ax.grid(True, linewidth=0.3, alpha=0.5, axis=\"y\")\n",
    "\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/Z500_and_ivt_som_representativeness.png\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved representativeness figure to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_representativeness.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86a127",
   "metadata": {},
   "source": [
    "### Composite Mean Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96757deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw composites\n",
    "z500_patterns_raw, _ = compute_composites(z500_ffe, bmus, xdim, ydim, time_dim=\"time\")\n",
    "ivt_patterns_raw, _ = compute_composites(\n",
    "    ivt_ffe, bmus, xdim, ydim, time_dim=\"valid_time\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8c92abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levels for raw composites\n",
    "levels_Z_raw = range(552, 595, 3)\n",
    "levels_ivt_raw = np.arange(0, 701, 100)\n",
    "\n",
    "fig, axes = create_som_figure(xdim, ydim)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # IVT shaded\n",
    "        im = ax.contourf(\n",
    "            lon,\n",
    "            lat,\n",
    "            ivt_patterns_raw[i, j],\n",
    "            cmap=\"BuPu\",\n",
    "            levels=levels_ivt_raw,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            extend=\"max\",\n",
    "        )\n",
    "\n",
    "        # Z500 contours\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            z500_patterns_raw[i, j] / 98.1,\n",
    "            colors=\"black\",\n",
    "            linewidths=0.5,\n",
    "            levels=levels_Z_raw,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "        )\n",
    "        ax.clabel(cn, inline=True, fontsize=5, fmt=\"%.0f\")\n",
    "\n",
    "        add_map_features(ax)\n",
    "        ax.set_title(f\"({i},{j})  N={counts[i, j]}\", fontsize=6)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "cbar.set_label(\"IVT (kg m$^{-1}$ s$^{-1}$)\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Flash Flood Only SOM Composite: IVT (shaded) + Z500 (contoured)\",\n",
    "    fontsize=8,\n",
    "    y=1.04,\n",
    ")\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/Z500_and_ivt_SOM_composite_mean_IVT_shaded.png\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e3a5f",
   "metadata": {},
   "source": [
    "### Maps of Individual Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "90749094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual IVT events per node\n",
    "plot_node_events(\n",
    "    ivt_ffe,\n",
    "    bmus,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    lon,\n",
    "    lat,\n",
    "    levels=np.arange(0, 1001, 100),\n",
    "    cmap=\"BuPu\",\n",
    "    save_pattern=\"figs/Z500-and-ivt-SOM/indiv-nodes/node_{i}_{j}.png\",\n",
    "    cbar_label=\"IVT (kg m$^{-1}$ s$^{-1}$)\",\n",
    "    z500_data=z500_ffe,\n",
    "    z500_levels=range(552, 595, 3),\n",
    "    z500_scale=1/98.1,\n",
    "    z500_time_dim=\"time\",\n",
    ")\n",
    "\n",
    "# Plot individual precipitation events per node\n",
    "plot_node_events(\n",
    "    tp_ffe,\n",
    "    bmus,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    lon,\n",
    "    lat,\n",
    "    levels=np.arange(0, 28, 3),\n",
    "    cmap=\"HomeyerRainbow\",\n",
    "    save_pattern=\"figs/Z500-and-ivt-SOM/indiv-nodes/node_{i}_{j}_precip.png\",\n",
    "    scale=1000,\n",
    "    cbar_label=\"Total Precipitation (mm)\",\n",
    ")\n",
    "\n",
    "# Plot individual MSLP events per node\n",
    "plot_node_events(\n",
    "    mslp_ffe,\n",
    "    bmus,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    lon,\n",
    "    lat,\n",
    "    levels=np.arange(976, 1041, 4),\n",
    "    cmap=None,\n",
    "    save_pattern=\"figs/ivt-SOM/indiv-nodes/node_{i}_{j}_mslp.png\",\n",
    "    scale=0.01,\n",
    "    contour=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9d344",
   "metadata": {},
   "source": [
    "### Maps of Composite MSLP for each SOM node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d6184f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSLP composites\n",
    "mslp_patterns, _ = compute_composites(mslp_ffe, bmus, xdim, ydim, time_dim=\"valid_time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "92c0e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = create_som_figure(xdim, ydim, figsize=(6, 3.7))\n",
    "\n",
    "levels = np.arange(1008, 1023, 2)\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        cn = ax.contour(\n",
    "            lon,\n",
    "            lat,\n",
    "            mslp_patterns[i, j] / 100,\n",
    "            levels=levels,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap=\"managua_r\",\n",
    "        )\n",
    "        add_map_features(ax)\n",
    "\n",
    "        # Add inline labels\n",
    "        ax.clabel(cn, cn.levels, fontsize=5, inline=True)\n",
    "        ax.set_title(f\"({i},{j})  N={counts[i, j]}\", fontsize=6)\n",
    "\n",
    "# cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6, pad=0.02)\n",
    "# cbar.set_label(\"MSLP (hPa)\", fontsize=6)\n",
    "\n",
    "plt.suptitle(\"SOM Composite MSLP\", fontsize=8, y=1.04)\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/z500_and_ivt_som_composite_mslp.png\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b872ef",
   "metadata": {},
   "source": [
    "### Month Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6f22dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute monthly event counts per node\n",
    "months = pd.to_datetime(ivt_ffe.valid_time).month.to_numpy()\n",
    "month_counts = {}\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        idx = get_node_indices(bmus, i, j)\n",
    "        node_months = months[idx]\n",
    "        month_counts[(i, j)] = np.bincount(node_months, minlength=13)[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4d05435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ydim, xdim, figsize=(6, 3.7), constrained_layout=True, dpi=600)\n",
    "\n",
    "# Warm-season labels\n",
    "month_labels = [\"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\"]\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # Extract May–Oct counts (months 5–10 → indices 4:10)\n",
    "        counts = month_counts[(i, j)][4:10]\n",
    "\n",
    "        ax.bar(month_labels, counts, color=\"teal\", alpha=0.9, width=0.8)\n",
    "\n",
    "        # Title matches your SOM composite style\n",
    "        ax.set_title(f\"({i},{j})  N={counts.sum()}\", fontsize=6)\n",
    "\n",
    "        # Remove ticks entirely (categorical labels don’t need them)\n",
    "        ax.tick_params(axis=\"x\", bottom=False, labelsize=5)\n",
    "\n",
    "        # Shared, fixed y-axis across all panels\n",
    "        ax.set_ylim(0, 18)\n",
    "        ax.set_yticks(np.arange(0, 17, 2))\n",
    "\n",
    "        # Light grid for readability\n",
    "        ax.grid(True, linewidth=0.3, alpha=0.5, axis=\"y\")\n",
    "\n",
    "# Overall title\n",
    "plt.suptitle(\n",
    "    \"Warm-Season (May–Oct) Event Distribution per SOM Node\", fontsize=8, y=1.04\n",
    ")\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/Z500_and_ivt_som_monthly_counts.png\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "04t1rpryi6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table (Node × Month):\n",
      "-------------------------------------------------------\n",
      "Node          May    Jun    Jul    Aug    Sep    Oct   Total\n",
      "-------------------------------------------------------\n",
      "(0,0)           1      5      6     10      3      0      25\n",
      "(0,1)           3      7      6      9      5      3      33\n",
      "(1,0)           3      5     16      5      4      2      35\n",
      "(1,1)           0      2      6     10      4      2      24\n",
      "-------------------------------------------------------\n",
      "Total           7     19     34     34     16      7     117\n",
      "\n",
      "=======================================================\n",
      "OVERALL CHI-SQUARE TEST FOR INDEPENDENCE\n",
      "=======================================================\n",
      "H₀: Monthly distributions are the same across all nodes\n",
      "H₁: At least one node has a different monthly distribution\n",
      "\n",
      "Chi-square statistic: 16.944\n",
      "Degrees of freedom:   15\n",
      "p-value:              0.3222\n",
      "\n",
      "Minimum expected count: 1.44\n",
      "Cells with expected < 5: 14/24\n",
      "⚠ Warning: >20% of cells have expected counts < 5; interpret with caution\n",
      "\n",
      "→ Result: FAIL TO REJECT H₀. No significant difference in monthly distributions.\n",
      "\n",
      "=======================================================\n",
      "PAIRWISE CHI-SQUARE TESTS (Bonferroni-corrected)\n",
      "=======================================================\n",
      "Number of comparisons: 6\n",
      "Bonferroni-corrected α: 0.0083\n",
      "\n",
      "(0,0) vs (0,1): χ²=3.86, p=0.5703 \n",
      "(0,0) vs (1,0): χ²=7.91, p=0.1614 \n",
      "(0,0) vs (1,1): χ²=4.41, p=0.4920 \n",
      "(0,1) vs (1,0): χ²=6.28, p=0.2800 \n",
      "(0,1) vs (1,1): χ²=4.84, p=0.4356 \n",
      "(1,0) vs (1,1): χ²=8.75, p=0.1194 \n",
      "\n",
      "No pairwise comparisons are significant after Bonferroni correction.\n"
     ]
    }
   ],
   "source": [
    "# Statistical test: Are the monthly distributions different across SOM nodes?\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "from itertools import combinations\n",
    "\n",
    "# Build contingency table: rows = nodes, columns = months (May-Oct)\n",
    "contingency = np.array(\n",
    "    [month_counts[(i, j)][4:10] for i in range(xdim) for j in range(ydim)]\n",
    ")\n",
    "\n",
    "node_labels = [f\"({i},{j})\" for i in range(xdim) for j in range(ydim)]\n",
    "month_labels = [\"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\"]\n",
    "\n",
    "# Display the contingency table\n",
    "print(\"Contingency Table (Node × Month):\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Node':<10}\", end=\"\")\n",
    "for m in month_labels:\n",
    "    print(f\"{m:>7}\", end=\"\")\n",
    "print(f\"{'Total':>8}\")\n",
    "print(\"-\" * 55)\n",
    "for k, label in enumerate(node_labels):\n",
    "    print(f\"{label:<10}\", end=\"\")\n",
    "    for val in contingency[k]:\n",
    "        print(f\"{val:>7}\", end=\"\")\n",
    "    print(f\"{contingency[k].sum():>8}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Total':<10}\", end=\"\")\n",
    "for val in contingency.sum(axis=0):\n",
    "    print(f\"{val:>7}\", end=\"\")\n",
    "print(f\"{contingency.sum():>8}\")\n",
    "print()\n",
    "\n",
    "# Chi-square test for independence (overall)\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"OVERALL CHI-SQUARE TEST FOR INDEPENDENCE\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"H₀: Monthly distributions are the same across all nodes\")\n",
    "print(f\"H₁: At least one node has a different monthly distribution\")\n",
    "print()\n",
    "print(f\"Chi-square statistic: {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom:   {dof}\")\n",
    "print(f\"p-value:              {p_value:.4f}\")\n",
    "print()\n",
    "\n",
    "# Check expected cell count assumption\n",
    "min_expected = expected.min()\n",
    "low_expected = (expected < 5).sum()\n",
    "print(f\"Minimum expected count: {min_expected:.2f}\")\n",
    "print(f\"Cells with expected < 5: {low_expected}/{expected.size}\")\n",
    "\n",
    "if low_expected > 0.2 * expected.size:\n",
    "    print(\"⚠ Warning: >20% of cells have expected counts < 5; interpret with caution\")\n",
    "print()\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"→ Result: REJECT H₀ at α=0.05. Monthly distributions differ across nodes.\")\n",
    "else:\n",
    "    print(\n",
    "        \"→ Result: FAIL TO REJECT H₀. No significant difference in monthly distributions.\"\n",
    "    )\n",
    "print()\n",
    "\n",
    "# Pairwise comparisons with Bonferroni correction\n",
    "print(\"=\" * 55)\n",
    "print(\"PAIRWISE CHI-SQUARE TESTS (Bonferroni-corrected)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "pairs = list(combinations(range(n_nodes), 2))\n",
    "n_comparisons = len(pairs)\n",
    "alpha_corrected = 0.05 / n_comparisons\n",
    "\n",
    "print(f\"Number of comparisons: {n_comparisons}\")\n",
    "print(f\"Bonferroni-corrected α: {alpha_corrected:.4f}\")\n",
    "print()\n",
    "\n",
    "pairwise_results = []\n",
    "for idx1, idx2 in pairs:\n",
    "    pair_table = contingency[[idx1, idx2], :]\n",
    "\n",
    "    # Use chi-square if expected counts are reasonable, otherwise note limitation\n",
    "    chi2_pair, p_pair, dof_pair, exp_pair = chi2_contingency(pair_table)\n",
    "\n",
    "    sig = \"***\" if p_pair < alpha_corrected else \"\"\n",
    "    pairwise_results.append(\n",
    "        {\n",
    "            \"pair\": f\"{node_labels[idx1]} vs {node_labels[idx2]}\",\n",
    "            \"chi2\": chi2_pair,\n",
    "            \"p\": p_pair,\n",
    "            \"sig\": p_pair < alpha_corrected,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"{node_labels[idx1]} vs {node_labels[idx2]}: χ²={chi2_pair:.2f}, p={p_pair:.4f} {sig}\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "significant_pairs = [r for r in pairwise_results if r[\"sig\"]]\n",
    "if significant_pairs:\n",
    "    print(f\"Significantly different pairs (p < {alpha_corrected:.4f}):\")\n",
    "    for r in significant_pairs:\n",
    "        print(f\"  • {r['pair']}\")\n",
    "else:\n",
    "    print(\"No pairwise comparisons are significant after Bonferroni correction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0f4c934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_totals = np.array(\n",
    "    [month_counts[(i, j)].sum() for i in range(xdim) for j in range(ydim)]\n",
    ")\n",
    "\n",
    "total_events = node_totals.sum()\n",
    "\n",
    "P_node = node_totals / total_events\n",
    "\n",
    "# Total events per month (all nodes combined)\n",
    "all_month_counts = np.bincount(months, minlength=13)[1:]  # 1–12\n",
    "\n",
    "# Only warm season (May–Oct)\n",
    "month_idx = np.arange(4, 10)  # indices for May–Oct\n",
    "\n",
    "n_nodes = xdim * ydim\n",
    "heatmap = np.zeros((n_nodes, len(month_idx)))\n",
    "\n",
    "# Flatten node indices consistently\n",
    "node_labels = []\n",
    "\n",
    "k = 0\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        counts = month_counts[(i, j)][month_idx]\n",
    "        totals = all_month_counts[month_idx]\n",
    "\n",
    "        # P(Node | Month)\n",
    "        heatmap[k, :] = counts / totals\n",
    "        node_labels.append(f\"({i},{j})\")\n",
    "\n",
    "        k += 1\n",
    "\n",
    "relative_heatmap = np.zeros_like(heatmap)\n",
    "\n",
    "for k in range(n_nodes):\n",
    "    relative_heatmap[k, :] = heatmap[k, :] / P_node[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1e3e44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3.7), dpi=600)\n",
    "\n",
    "im = ax.imshow(relative_heatmap, aspect=\"auto\", cmap=\"RdBu_r\", vmin=0, vmax=2)\n",
    "\n",
    "# Axes labels\n",
    "ax.set_xticks(np.arange(len(month_idx)))\n",
    "ax.set_xticklabels([\"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\"], fontsize=7)\n",
    "\n",
    "ax.set_yticks(np.arange(n_nodes))\n",
    "ax.set_yticklabels(node_labels, fontsize=6)\n",
    "\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"SOM Node\")\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Relative Likelihood\", fontsize=7)\n",
    "\n",
    "plt.title(\n",
    "    \"Monthly Relative Likelihood of SOM Nodes\\n\"\n",
    "    \"(Normalized by Seasonal Event Frequency)\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/Z500_and_ivt_som_monthly_relative_heatmap.png\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1c61d",
   "metadata": {},
   "source": [
    "## Node Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a84df8",
   "metadata": {},
   "source": [
    "### Maximum Hourly Rainfall by SOM Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4efxa5h6r69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precipitation data for 4 sites\n",
      "  JFK: 1949-01-05 08:00:00 to 2025-07-14 23:00:00, 670792 records\n",
      "  LGA: 1948-07-05 22:00:00 to 2025-07-14 22:00:00, 675193 records\n",
      "  Central Park: 1948-05-03 00:00:00 to 2025-07-14 22:00:00, 676727 records\n",
      "  EWR: 1948-05-02 23:00:00 to 2025-07-14 23:00:00, 676729 records\n"
     ]
    }
   ],
   "source": [
    "# Load hourly precipitation data from four NYC-area sites\n",
    "precip_path = \"precip_data_and_tc_association_code/\"\n",
    "sites = {\n",
    "    \"JFK\": np.load(f\"{precip_path}jfk7_14_25.npy\", allow_pickle=True),\n",
    "    \"LGA\": np.load(f\"{precip_path}lga7_14_25.npy\", allow_pickle=True),\n",
    "    \"Central Park\": np.load(f\"{precip_path}cp7_14_25.npy\", allow_pickle=True),\n",
    "    \"EWR\": np.load(f\"{precip_path}zewr_14_25.npy\", allow_pickle=True),\n",
    "}\n",
    "\n",
    "# Convert to DataFrames with datetime index for easy lookup\n",
    "precip_dfs = {}\n",
    "for name, arr in sites.items():\n",
    "    df = pd.DataFrame(arr, columns=[\"precip\", \"time\"])\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.set_index(\"time\").sort_index()\n",
    "    df[\"precip\"] = pd.to_numeric(df[\"precip\"], errors=\"coerce\")\n",
    "    precip_dfs[name] = df\n",
    "\n",
    "print(f\"Loaded precipitation data for {len(precip_dfs)} sites\")\n",
    "for name, df in precip_dfs.items():\n",
    "    print(f\"  {name}: {df.index.min()} to {df.index.max()}, {len(df)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dptyf8561ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found precipitation data for 116/117 events\n",
      "Max hourly rainfall range: 0.06 - 3.62 inches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1072982/4215835900.py:29: RuntimeWarning: All-NaN axis encountered\n",
      "  max_precip.append(np.nanmax(site_maxes) if site_maxes else np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Load BMU assignments (timestamps are in UTC)\n",
    "bmu_df = pd.read_csv(\"data/som_2x2_bmus.csv\")\n",
    "bmu_df[\"timestamp\"] = pd.to_datetime(bmu_df[\"timestamp\"])\n",
    "\n",
    "# Convert UTC timestamps to local time for matching with precip data\n",
    "# Precip data is in local time (EST/EDT)\n",
    "bmu_df[\"timestamp_local\"] = (\n",
    "    bmu_df[\"timestamp\"].dt.tz_localize(\"UTC\").dt.tz_convert(\"EST\").dt.tz_localize(None)\n",
    ")\n",
    "\n",
    "# For each event, find the max hourly rainfall across all four sites\n",
    "# Look at a 12-hour window centered on the event time to capture peak rainfall\n",
    "window_hours = 6  # hours before and after\n",
    "\n",
    "max_precip = []\n",
    "for _, row in bmu_df.iterrows():\n",
    "    event_time = row[\"timestamp_local\"]\n",
    "    start = event_time - pd.Timedelta(hours=window_hours)\n",
    "    end = event_time + pd.Timedelta(hours=window_hours)\n",
    "\n",
    "    # Get max precip at each site within the window\n",
    "    site_maxes = []\n",
    "    for name, df in precip_dfs.items():\n",
    "        window_data = df.loc[start:end, \"precip\"]\n",
    "        if len(window_data) > 0:\n",
    "            site_maxes.append(window_data.max())\n",
    "\n",
    "    # Take max across all sites\n",
    "    max_precip.append(np.nanmax(site_maxes) if site_maxes else np.nan)\n",
    "\n",
    "bmu_df[\"max_precip_in\"] = max_precip\n",
    "\n",
    "# Report coverage\n",
    "valid = bmu_df[\"max_precip_in\"].notna().sum()\n",
    "print(f\"Found precipitation data for {valid}/{len(bmu_df)} events\")\n",
    "print(\n",
    "    f\"Max hourly rainfall range: {bmu_df['max_precip_in'].min():.2f} - {bmu_df['max_precip_in'].max():.2f} inches\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b7347e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NYC-Area Max Hourly Precipitation Statistics by SOM Node:\n",
      "Node (i,j) | Count | Mean (in) | Median (in) | Std Dev (in)\n",
      "   (0,0)   |   25  |  1.09   |   1.01   |   0.48\n",
      "   (0,1)   |   32  |  0.88   |   0.80   |   0.56\n",
      "   (1,0)   |   35  |  1.06   |   1.03   |   0.48\n",
      "   (1,1)   |   24  |  0.88   |   0.78   |   0.71\n"
     ]
    }
   ],
   "source": [
    "# Check counts for each node pair (i,j)\n",
    "node_precip_stats = {}\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node_data = bmu_df[(bmu_df[\"node_i\"] == i) & (bmu_df[\"node_j\"] == j)]\n",
    "        precip_values = node_data[\"max_precip_in\"].dropna().values\n",
    "\n",
    "        if len(precip_values) > 0:\n",
    "            mean_precip = np.mean(precip_values)\n",
    "            median_precip = np.median(precip_values)\n",
    "            std_precip = np.std(precip_values)\n",
    "        else:\n",
    "            mean_precip = median_precip = std_precip = np.nan\n",
    "\n",
    "        node_precip_stats[(i, j)] = {\n",
    "            \"count\": len(precip_values),\n",
    "            \"mean\": mean_precip,\n",
    "            \"median\": median_precip,\n",
    "            \"std\": std_precip,\n",
    "        }\n",
    "# Print summary table\n",
    "print(\"\\nNYC-Area Max Hourly Precipitation Statistics by SOM Node:\")\n",
    "print(\"Node (i,j) | Count | Mean (in) | Median (in) | Std Dev (in)\")\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        stats = node_precip_stats[(i, j)]\n",
    "        print(\n",
    "            f\"   ({i},{j})   |  {stats['count']:3d}  |  {stats['mean']:.2f}   |   {stats['median']:.2f}   |   {stats['std']:.2f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "moahh6zt6zl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved histogram to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_max_precip_histograms.png\n"
     ]
    }
   ],
   "source": [
    "# Create histogram of max hourly rainfall for each SOM node\n",
    "fig, axes = plt.subplots(ydim, xdim, figsize=(6, 4), constrained_layout=True, dpi=600)\n",
    "\n",
    "# Define consistent bins for all histograms (in inches)\n",
    "bins = np.arange(0, 3.76, 0.25)\n",
    "\n",
    "# Colors for each node (matching other plots)\n",
    "colors = [\"steelblue\", \"darkorange\", \"seagreen\", \"firebrick\"]\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        # Filter events for this node\n",
    "        node_data = bmu_df[(bmu_df[\"node_i\"] == i) & (bmu_df[\"node_j\"] == j)][\n",
    "            \"max_precip_in\"\n",
    "        ].dropna()\n",
    "\n",
    "        # Plot histogram\n",
    "        ax.hist(\n",
    "            node_data,\n",
    "            bins=bins,\n",
    "            color=\"teal\",\n",
    "            alpha=0.9,\n",
    "            edgecolor=\"white\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "        # Add statistics\n",
    "        n = len(node_data)\n",
    "        median = node_data.median() if n > 0 else np.nan\n",
    "        mean = node_data.mean() if n > 0 else np.nan\n",
    "\n",
    "        # Add vertical line for median\n",
    "        if n > 0:\n",
    "            ax.axvline(\n",
    "                median,\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=1,\n",
    "                label=f'Median: {median:.2f}\"',\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"({i},{j})  N={n}\", fontsize=6)\n",
    "        ax.set_xlim(0, 3.75)\n",
    "        ax.set_ylim(0, 12)\n",
    "        ax.set_yticks(np.arange(0, 13, 2))\n",
    "        ax.tick_params(axis=\"both\", labelsize=5)\n",
    "        ax.grid(True, linewidth=0.3, alpha=0.5, axis=\"y\")\n",
    "\n",
    "        # Only add x-label on bottom row\n",
    "        if j == ydim - 1:\n",
    "            ax.set_xlabel(\"Max Hourly Precip (in)\", fontsize=5)\n",
    "\n",
    "        # Only add y-label on left column\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Count\", fontsize=5)\n",
    "\n",
    "        # Add legend with median\n",
    "        if n > 0:\n",
    "            ax.legend(fontsize=4, loc=\"upper right\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Maximum Hourly Rainfall ($\\\\pm$6 hr window) by SOM Node\",\n",
    "    fontsize=8,\n",
    "    y=1.02,\n",
    ")\n",
    "plt.savefig(\n",
    "    \"figs/Z500-and-ivt-SOM/Z500_and_ivt_som_max_precip_histograms.png\",\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\n",
    "    \"Saved histogram to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_max_precip_histograms.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15aa511",
   "metadata": {},
   "source": [
    "### Tropical Cyclone Association by SOM Node\n",
    "\n",
    "We cross-reference flash flood event times with IBTrACS to identify events where a tropical system was present within the analysis domain (30–54°N, 100–60°W). This helps determine whether certain SOM patterns are preferentially associated with tropical cyclone activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b17matk6iwj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IBTrACS records: 64,320\n",
      "Records within domain: 12,035\n",
      "Unique storms in domain: 1185\n"
     ]
    }
   ],
   "source": [
    "# Load IBTrACS data\n",
    "ibtracs_path = \"precip_data_and_tc_association_code/ibtracs.NA.list.v04r01.processed_6hrly.statslp3.csv\"\n",
    "ibtracs = pd.read_csv(ibtracs_path)\n",
    "ibtracs[\"ISO_TIME\"] = pd.to_datetime(ibtracs[\"ISO_TIME\"])\n",
    "\n",
    "# Define domain bounds (same as IVT/Z500 data)\n",
    "lat_min, lat_max = 30.0, 54.0\n",
    "lon_min, lon_max = -100.0, -60.0\n",
    "\n",
    "# Filter IBTrACS to our domain\n",
    "ibtracs_domain = ibtracs[\n",
    "    (ibtracs[\"LAT\"] >= lat_min)\n",
    "    & (ibtracs[\"LAT\"] <= lat_max)\n",
    "    & (ibtracs[\"LON\"] >= lon_min)\n",
    "    & (ibtracs[\"LON\"] <= lon_max)\n",
    "].copy()\n",
    "\n",
    "print(f\"Total IBTrACS records: {len(ibtracs):,}\")\n",
    "print(f\"Records within domain: {len(ibtracs_domain):,}\")\n",
    "print(f\"Unique storms in domain: {ibtracs_domain['SID'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "his457vdtkh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash flood events associated with TCs: 22/117 (18.8%)\n",
      "\n",
      "TC-associated events by SOM node:\n",
      "  Node (0,0): 5/25 (20.0%)\n",
      "  Node (0,1): 7/33 (21.2%)\n",
      "  Node (1,0): 7/35 (20.0%)\n",
      "  Node (1,1): 3/24 (12.5%)\n"
     ]
    }
   ],
   "source": [
    "# Cross-reference flash flood events with tropical cyclones\n",
    "# Look for TCs within the domain within ±12 hours of each event\n",
    "\n",
    "time_window_hours = 6\n",
    "\n",
    "tc_associations = []\n",
    "for _, row in bmu_df.iterrows():\n",
    "    event_time = row[\"timestamp\"]\n",
    "    node_i, node_j = row[\"node_i\"], row[\"node_j\"]\n",
    "\n",
    "    # Find TCs within time window\n",
    "    time_mask = (\n",
    "        ibtracs_domain[\"ISO_TIME\"] >= event_time - pd.Timedelta(hours=time_window_hours)\n",
    "    ) & (\n",
    "        ibtracs_domain[\"ISO_TIME\"] <= event_time + pd.Timedelta(hours=time_window_hours)\n",
    "    )\n",
    "    matching_tcs = ibtracs_domain[time_mask]\n",
    "\n",
    "    if len(matching_tcs) > 0:\n",
    "        # Get unique storm IDs and names\n",
    "        storm_ids = matching_tcs[\"SID\"].unique()\n",
    "        tc_associations.append(\n",
    "            {\n",
    "                \"timestamp\": event_time,\n",
    "                \"node_i\": node_i,\n",
    "                \"node_j\": node_j,\n",
    "                \"tc_present\": True,\n",
    "                \"n_storms\": len(storm_ids),\n",
    "                \"storm_ids\": \", \".join(storm_ids),\n",
    "                \"storm_status\": matching_tcs[\"STAT\"].mode().iloc[0]\n",
    "                if len(matching_tcs[\"STAT\"].dropna()) > 0\n",
    "                else \"Unknown\",\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        tc_associations.append(\n",
    "            {\n",
    "                \"timestamp\": event_time,\n",
    "                \"node_i\": node_i,\n",
    "                \"node_j\": node_j,\n",
    "                \"tc_present\": False,\n",
    "                \"n_storms\": 0,\n",
    "                \"storm_ids\": \"\",\n",
    "                \"storm_status\": \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "tc_df = pd.DataFrame(tc_associations)\n",
    "\n",
    "# Summary statistics\n",
    "n_tc_events = tc_df[\"tc_present\"].sum()\n",
    "print(\n",
    "    f\"Flash flood events associated with TCs: {n_tc_events}/{len(tc_df)} ({100 * n_tc_events / len(tc_df):.1f}%)\"\n",
    ")\n",
    "print(f\"\\nTC-associated events by SOM node:\")\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node_data = tc_df[(tc_df[\"node_i\"] == i) & (tc_df[\"node_j\"] == j)]\n",
    "        tc_count = node_data[\"tc_present\"].sum()\n",
    "        total = len(node_data)\n",
    "        pct = 100 * tc_count / total if total > 0 else 0\n",
    "        print(f\"  Node ({i},{j}): {tc_count}/{total} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "48j9g5sbo0l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TC-Associated Flash Flood Events:\n",
      "--------------------------------------------------------------------------------\n",
      "1996-07-13 13:00 | Node (1,0) | Status:  TS | 1996187N10326\n",
      "1996-09-08 20:00 | Node (1,1) | Status:  EX | 1996237N14339\n",
      "1997-07-16 00:00 | Node (0,1) | Status:  TS | 1997194N31286\n",
      "1999-09-16 16:00 | Node (1,0) | Status:  HU | 1999251N15314\n",
      "2001-06-17 15:00 | Node (0,0) | Status:  SS | 2001157N28265\n",
      "2002-09-02 13:00 | Node (0,1) | Status:  TS | 2002245N29281\n",
      "2004-09-08 10:00 | Node (0,0) | Status:  TD | 2004238N11325\n",
      "2004-09-18 13:00 | Node (0,1) | Status:  EX | 2004247N10332\n",
      "2004-09-28 21:00 | Node (0,1) | Status:  EX | 2004258N16300\n",
      "2005-07-06 23:00 | Node (1,1) | Status:  TD | 2005185N18273\n",
      "2005-10-14 21:00 | Node (0,1) | Status:  EX | 2005281N26303\n",
      "2006-07-21 21:00 | Node (0,1) | Status:  EX | 2006200N32287\n",
      "2007-06-04 13:00 | Node (1,0) | Status:  EX | 2007151N18273\n",
      "2008-09-06 22:00 | Node (1,0) | Status:  TS | 2008241N19303\n",
      "2014-07-03 00:00 | Node (1,0) | Status:  HU | 2014180N32282\n",
      "2014-07-04 00:00 | Node (1,0) | Status:  HU | 2014180N32282\n",
      "2020-07-10 17:00 | Node (0,0) | Status:  TS | 2020188N28271\n",
      "2021-07-08 19:00 | Node (1,0) | Status:  TS | 2021182N09317\n",
      "2021-08-22 02:00 | Node (0,0) | Status:  HU | 2021227N36297\n",
      "2021-08-22 19:00 | Node (0,1) | Status:  TD | 2021227N36297\n",
      "2021-09-01 22:00 | Node (1,1) | Status:  EX | 2021239N17281\n",
      "2024-08-06 21:00 | Node (0,0) | Status:  TS | 2024214N18298\n"
     ]
    }
   ],
   "source": [
    "# List TC-associated flash flood events\n",
    "tc_events = tc_df[tc_df[\"tc_present\"]].copy()\n",
    "tc_events = tc_events.sort_values(\"timestamp\")\n",
    "\n",
    "print(\"TC-Associated Flash Flood Events:\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in tc_events.iterrows():\n",
    "    print(\n",
    "        f\"{row['timestamp'].strftime('%Y-%m-%d %H:%M')} | \"\n",
    "        f\"Node ({row['node_i']},{row['node_j']}) | \"\n",
    "        f\"Status: {row['storm_status']:>3} | \"\n",
    "        f\"{row['storm_ids']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7no1lu2zxui",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TC association figure to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_tc_association.png\n"
     ]
    }
   ],
   "source": [
    "# Create bar chart showing TC vs non-TC events by SOM node\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3.5), dpi=600, constrained_layout=True)\n",
    "\n",
    "# Compute counts per node\n",
    "tc_counts = np.zeros((xdim, ydim))\n",
    "non_tc_counts = np.zeros((xdim, ydim))\n",
    "\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node_data = tc_df[(tc_df[\"node_i\"] == i) & (tc_df[\"node_j\"] == j)]\n",
    "        tc_counts[i, j] = node_data[\"tc_present\"].sum()\n",
    "        non_tc_counts[i, j] = (~node_data[\"tc_present\"]).sum()\n",
    "\n",
    "# Left panel: Stacked bar chart\n",
    "ax = axes[0]\n",
    "node_labels = [f\"({i},{j})\" for i in range(xdim) for j in range(ydim)]\n",
    "x = np.arange(len(node_labels))\n",
    "width = 0.6\n",
    "\n",
    "tc_flat = tc_counts.flatten()\n",
    "non_tc_flat = non_tc_counts.flatten()\n",
    "\n",
    "bars1 = ax.bar(x, non_tc_flat, width, label=\"Non-TC\", color=\"steelblue\", alpha=0.9)\n",
    "bars2 = ax.bar(x, tc_flat, width, bottom=non_tc_flat, label=\"TC-Associated\", color=\"coral\", alpha=0.9)\n",
    "\n",
    "ax.set_xlabel(\"SOM Node\", fontsize=7)\n",
    "ax.set_ylabel(\"Number of Events\", fontsize=7)\n",
    "ax.set_title(\"Flash Flood Events by TC Association\", fontsize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(node_labels, fontsize=6)\n",
    "ax.legend(fontsize=6, loc=\"upper right\")\n",
    "ax.grid(True, linewidth=0.3, alpha=0.5, axis=\"y\")\n",
    "\n",
    "# Right panel: TC percentage by node\n",
    "ax = axes[1]\n",
    "totals = tc_flat + non_tc_flat\n",
    "tc_pct = 100 * tc_flat / totals\n",
    "\n",
    "bars = ax.bar(x, tc_pct, width, color=\"coral\", alpha=0.9, edgecolor=\"white\")\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, pct, tc, total in zip(bars, tc_pct, tc_flat, totals):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        f\"{int(tc)}/{int(total)}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "# Add overall average line\n",
    "overall_pct = 100 * tc_flat.sum() / totals.sum()\n",
    "ax.axhline(overall_pct, color=\"red\", linestyle=\"--\", linewidth=1, label=f\"Overall: {overall_pct:.1f}\\\\%\")\n",
    "\n",
    "ax.set_xlabel(\"SOM Node\", fontsize=7)\n",
    "ax.set_ylabel(\"TC-Associated Events (\\\\%)\", fontsize=7)\n",
    "ax.set_title(\"Percentage of Events with TC Influence\", fontsize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(node_labels, fontsize=6)\n",
    "ax.set_ylim(0, max(tc_pct) + 15)\n",
    "ax.legend(fontsize=6, loc=\"upper right\")\n",
    "ax.grid(True, linewidth=0.3, alpha=0.5, axis=\"y\")\n",
    "\n",
    "plt.savefig(\"figs/Z500-and-ivt-SOM/Z500_and_ivt_som_tc_association.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved TC association figure to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_tc_association.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "l0dck0h21v9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHI-SQUARE TEST: TC Association vs SOM Node\n",
      "============================================================\n",
      "\n",
      "Contingency Table:\n",
      "            (0,0):>8(0,1):>8(1,0):>8(1,1):>8   Total\n",
      "          TC       5       7       7       3      22\n",
      "      Non-TC      20      26      28      21      95\n",
      "       Total      25      33      35      24     117\n",
      "\n",
      "Chi-square statistic: 0.806\n",
      "Degrees of freedom:   3\n",
      "p-value:              0.8480\n",
      "\n",
      "Minimum expected count: 4.51\n",
      "⚠ Warning: Some expected counts < 5; interpret with caution\n",
      "\n",
      "→ Result: FAIL TO REJECT H₀. No significant difference in TC association across nodes.\n",
      "\n",
      "============================================================\n",
      "PAIRWISE FISHER'S EXACT TESTS (Bonferroni-corrected)\n",
      "============================================================\n",
      "Number of comparisons: 6\n",
      "Bonferroni-corrected α: 0.0083\n",
      "\n",
      "(0,0) vs (0,1): OR=0.93, p=1.0000 \n",
      "(0,0) vs (1,0): OR=1.00, p=1.0000 \n",
      "(0,0) vs (1,1): OR=1.75, p=0.7019 \n",
      "(0,1) vs (1,0): OR=1.08, p=1.0000 \n",
      "(0,1) vs (1,1): OR=1.88, p=0.4939 \n",
      "(1,0) vs (1,1): OR=1.75, p=0.5059 \n"
     ]
    }
   ],
   "source": [
    "# Statistical test: Is TC association different across SOM nodes?\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "\n",
    "# Build 2xN contingency table: rows = TC/non-TC, columns = nodes\n",
    "contingency_tc = np.array([tc_flat, non_tc_flat])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHI-SQUARE TEST: TC Association vs SOM Node\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nContingency Table:\")\n",
    "print(f\"{'':>12}\", end=\"\")\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        print(f\"({i},{j}):>8\", end=\"\")\n",
    "print(f\"{'Total':>8}\")\n",
    "\n",
    "print(f\"{'TC':>12}\", end=\"\")\n",
    "for val in tc_flat:\n",
    "    print(f\"{int(val):>8}\", end=\"\")\n",
    "print(f\"{int(tc_flat.sum()):>8}\")\n",
    "\n",
    "print(f\"{'Non-TC':>12}\", end=\"\")\n",
    "for val in non_tc_flat:\n",
    "    print(f\"{int(val):>8}\", end=\"\")\n",
    "print(f\"{int(non_tc_flat.sum()):>8}\")\n",
    "\n",
    "print(f\"{'Total':>12}\", end=\"\")\n",
    "for val in totals:\n",
    "    print(f\"{int(val):>8}\", end=\"\")\n",
    "print(f\"{int(totals.sum()):>8}\")\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_tc)\n",
    "\n",
    "print(f\"\\nChi-square statistic: {chi2:.3f}\")\n",
    "print(f\"Degrees of freedom:   {dof}\")\n",
    "print(f\"p-value:              {p_value:.4f}\")\n",
    "\n",
    "# Check expected cell counts\n",
    "min_expected = expected.min()\n",
    "print(f\"\\nMinimum expected count: {min_expected:.2f}\")\n",
    "\n",
    "if min_expected < 5:\n",
    "    print(\"⚠ Warning: Some expected counts < 5; interpret with caution\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\n→ Result: REJECT H₀ at α=0.05. TC association differs across SOM nodes.\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\n→ Result: FAIL TO REJECT H₀. No significant difference in TC association across nodes.\"\n",
    "    )\n",
    "\n",
    "# Pairwise Fisher's exact tests\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PAIRWISE FISHER'S EXACT TESTS (Bonferroni-corrected)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pairs = list(combinations(range(n_nodes), 2))\n",
    "n_comparisons = len(pairs)\n",
    "alpha_corrected = 0.05 / n_comparisons\n",
    "\n",
    "print(f\"Number of comparisons: {n_comparisons}\")\n",
    "print(f\"Bonferroni-corrected α: {alpha_corrected:.4f}\\n\")\n",
    "\n",
    "for idx1, idx2 in pairs:\n",
    "    # 2x2 table for this pair\n",
    "    table = np.array(\n",
    "        [[tc_flat[idx1], tc_flat[idx2]], [non_tc_flat[idx1], non_tc_flat[idx2]]]\n",
    "    )\n",
    "    odds_ratio, p = fisher_exact(table)\n",
    "    sig = \"***\" if p < alpha_corrected else \"\"\n",
    "    print(\n",
    "        f\"{node_labels[idx1]} vs {node_labels[idx2]}: OR={odds_ratio:.2f}, p={p:.4f} {sig}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "k4yb3dh4s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TC tracks map to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_tc_tracks.png\n"
     ]
    }
   ],
   "source": [
    "# Map showing TC positions during flash flood events, colored by SOM node\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(8, 5),\n",
    "    subplot_kw={\"projection\": ccrs.PlateCarree()},\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "# Define colors for each node\n",
    "node_colors = {\n",
    "    (0, 0): \"tab:blue\",\n",
    "    (0, 1): \"tab:orange\",\n",
    "    (1, 0): \"tab:green\",\n",
    "    (1, 1): \"tab:red\",\n",
    "}\n",
    "\n",
    "# Plot domain box\n",
    "ax.plot(\n",
    "    [lon_min, lon_max, lon_max, lon_min, lon_min],\n",
    "    [lat_min, lat_min, lat_max, lat_max, lat_min],\n",
    "    \"k--\",\n",
    "    linewidth=1,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    label=\"Analysis Domain\",\n",
    ")\n",
    "\n",
    "# For each TC-associated event, plot the TC track segment\n",
    "for _, row in tc_events.iterrows():\n",
    "    event_time = row[\"timestamp\"]\n",
    "    node_i, node_j = int(row[\"node_i\"]), int(row[\"node_j\"])\n",
    "    storm_ids = row[\"storm_ids\"].split(\", \")\n",
    "\n",
    "    for sid in storm_ids:\n",
    "        # Get track for this storm around event time\n",
    "        storm_data = ibtracs[ibtracs[\"SID\"] == sid].copy()\n",
    "        storm_data = storm_data[\n",
    "            (storm_data[\"ISO_TIME\"] >= event_time - pd.Timedelta(hours=48))\n",
    "            & (storm_data[\"ISO_TIME\"] <= event_time + pd.Timedelta(hours=48))\n",
    "        ]\n",
    "\n",
    "        if len(storm_data) > 1:\n",
    "            ax.plot(\n",
    "                storm_data[\"LON\"],\n",
    "                storm_data[\"LAT\"],\n",
    "                color=node_colors[(node_i, node_j)],\n",
    "                linewidth=1.5,\n",
    "                alpha=0.7,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "            )\n",
    "\n",
    "        # Mark position at event time (closest 6-hourly fix)\n",
    "        closest_idx = (storm_data[\"ISO_TIME\"] - event_time).abs().idxmin()\n",
    "        closest = storm_data.loc[closest_idx]\n",
    "        ax.scatter(\n",
    "            closest[\"LON\"],\n",
    "            closest[\"LAT\"],\n",
    "            color=node_colors[(node_i, node_j)],\n",
    "            s=40,\n",
    "            marker=\"o\",\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            zorder=5,\n",
    "        )\n",
    "\n",
    "# Add NYC marker\n",
    "ax.scatter(\n",
    "    -74.0,\n",
    "    40.7,\n",
    "    color=\"black\",\n",
    "    s=100,\n",
    "    marker=\"*\",\n",
    "    zorder=10,\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "ax.text(-73.5, 40.7, \"NYC\", fontsize=7, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Map features\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linewidth=0.3)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "ax.set_extent([lon_min - 5, lon_max + 5, lat_min - 5, lat_max + 5])\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D(\n",
    "        [0], [0], color=node_colors[(i, j)], linewidth=2, label=f\"Node ({i},{j})\"\n",
    "    )\n",
    "    for i in range(xdim)\n",
    "    for j in range(ydim)\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=6)\n",
    "\n",
    "ax.set_title(\n",
    "    \"TC Tracks During Flash Flood Events\\n(±48 hr track segments, markers at event time)\",\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "plt.savefig(\"figs/Z500-and-ivt-SOM/Z500_and_ivt_som_tc_tracks.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved TC tracks map to figs/Z500-and-ivt-SOM/Z500_and_ivt_som_tc_tracks.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "oeffpgywxlo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precipitation Statistics: TC vs Non-TC Events\n",
      "=================================================================\n",
      "Category                  N    Mean (in)  Median (in)   Std (in)\n",
      "-----------------------------------------------------------------\n",
      "TC-Associated            22         1.17         1.02       0.77\n",
      "Non-TC                   94         0.94         0.89       0.50\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Mann-Whitney U test: U=1197.5, p=0.2510\n",
      "→ No significant difference in precipitation intensity\n"
     ]
    }
   ],
   "source": [
    "# Compare precipitation intensity for TC vs non-TC events\n",
    "# Merge TC info with precipitation data\n",
    "bmu_df_with_tc = bmu_df.merge(\n",
    "    tc_df[[\"timestamp\", \"tc_present\", \"storm_ids\"]],\n",
    "    on=\"timestamp\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Precipitation Statistics: TC vs Non-TC Events\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Category':<20} {'N':>6} {'Mean (in)':>12} {'Median (in)':>12} {'Std (in)':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Overall comparison\n",
    "tc_precip = bmu_df_with_tc[bmu_df_with_tc[\"tc_present\"]][\"max_precip_in\"].dropna()\n",
    "non_tc_precip = bmu_df_with_tc[~bmu_df_with_tc[\"tc_present\"]][\"max_precip_in\"].dropna()\n",
    "\n",
    "print(f\"{'TC-Associated':<20} {len(tc_precip):>6} {tc_precip.mean():>12.2f} {tc_precip.median():>12.2f} {tc_precip.std():>10.2f}\")\n",
    "print(f\"{'Non-TC':<20} {len(non_tc_precip):>6} {non_tc_precip.mean():>12.2f} {non_tc_precip.median():>12.2f} {non_tc_precip.std():>10.2f}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Mann-Whitney U test for difference\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "if len(tc_precip) > 0 and len(non_tc_precip) > 0:\n",
    "    stat, p = mannwhitneyu(tc_precip, non_tc_precip, alternative=\"two-sided\")\n",
    "    print(f\"\\nMann-Whitney U test: U={stat:.1f}, p={p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(\"→ Significant difference in precipitation intensity between TC and non-TC events\")\n",
    "    else:\n",
    "        print(\"→ No significant difference in precipitation intensity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "vkei5dbx6ol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TC association data to data/som_2x2_tc_associations.csv\n",
      "\n",
      "============================================================\n",
      "SUMMARY: Tropical Cyclone Association with SOM Nodes\n",
      "============================================================\n",
      "\n",
      "Total flash flood events: 117\n",
      "TC-associated events: 22 (18.8%)\n",
      "\n",
      "TC association by node:\n",
      "  Node (0,0):  5/25 ( 20.0%)\n",
      "  Node (0,1):  7/33 ( 21.2%)\n",
      "  Node (1,0):  7/35 ( 20.0%)\n",
      "  Node (1,1):  3/24 ( 12.5%)\n"
     ]
    }
   ],
   "source": [
    "# Save TC association data for future reference\n",
    "tc_df.to_csv(\"data/som_2x2_tc_associations.csv\", index=False)\n",
    "print(f\"Saved TC association data to data/som_2x2_tc_associations.csv\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY: Tropical Cyclone Association with SOM Nodes\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal flash flood events: {len(tc_df)}\")\n",
    "print(f\"TC-associated events: {n_tc_events} ({100*n_tc_events/len(tc_df):.1f}%)\")\n",
    "print(f\"\\nTC association by node:\")\n",
    "for i in range(xdim):\n",
    "    for j in range(ydim):\n",
    "        node_data = tc_df[(tc_df[\"node_i\"] == i) & (tc_df[\"node_j\"] == j)]\n",
    "        tc_count = node_data[\"tc_present\"].sum()\n",
    "        total = len(node_data)\n",
    "        pct = 100 * tc_count / total if total > 0 else 0\n",
    "        print(f\"  Node ({i},{j}): {tc_count:2d}/{total:2d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e7d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soms314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
